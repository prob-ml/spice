{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a296b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "import json\n",
    "import itertools as it\n",
    "import pathlib\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from sklearn import neighbors\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "class MerfishDataset(torch_geometric.data.InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "    ):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # non-response genes (columns) in MERFISH\n",
    "        with open(non_response_genes_file, \"r\", encoding=\"utf8\") as genes_file:\n",
    "            self.features = [int(x) for x in genes_file.read().split(\",\")]\n",
    "            genes_file.close()\n",
    "\n",
    "        # response genes (columns in MERFISH)\n",
    "        self.response_genes = list(set(range(155)) - set(self.features))\n",
    "\n",
    "        data_list = self.construct_graphs(\n",
    "            n_neighbors, train, log_transform, neighbor_celltypes, radius\n",
    "        )\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            self.gene_names = h5f[\"gene_names\"][:][~self.bad_genes].astype(\"U\")\n",
    "\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    # from https://datadryad.org/stash/dataset/doi:10.5061/dryad.8t8s248\n",
    "    url = \"https://datadryad.org/stash/downloads/file_stream/67671\"\n",
    "\n",
    "    behavior_types = [\n",
    "        \"Naive\",\n",
    "        \"Parenting\",\n",
    "        \"Virgin Parenting\",\n",
    "        \"Aggression to pup\",\n",
    "        \"Aggression to adult\",\n",
    "        \"Mating\",\n",
    "    ]\n",
    "    behavior_lookup = {x: i for (i, x) in enumerate(behavior_types)}\n",
    "    cell_types = [\n",
    "        \"Ambiguous\",\n",
    "        \"Astrocyte\",\n",
    "        \"Endothelial 1\",\n",
    "        \"Endothelial 2\",\n",
    "        \"Endothelial 3\",\n",
    "        \"Ependymal\",\n",
    "        \"Excitatory\",\n",
    "        \"Inhibitory\",\n",
    "        \"Microglia\",\n",
    "        \"OD Immature 1\",\n",
    "        \"OD Immature 2\",\n",
    "        \"OD Mature 1\",\n",
    "        \"OD Mature 2\",\n",
    "        \"OD Mature 3\",\n",
    "        \"OD Mature 4\",\n",
    "        \"Pericytes\",\n",
    "    ]\n",
    "    celltype_lookup = {x: i for (i, x) in enumerate(cell_types)}\n",
    "\n",
    "    bad_genes = np.zeros(161, dtype=bool)\n",
    "    bad_genes[[12, 13, 14, 15, 16, 144]] = True\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"merfish.csv\", \"merfish.hdf5\"]\n",
    "\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.csv\")\n",
    "\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.hdf5\")\n",
    "\n",
    "    def download(self):\n",
    "        # download csv if necessary\n",
    "        if not os.path.exists(self.merfish_csv):\n",
    "            with open(self.merfish_csv, \"wb\") as csvf:\n",
    "                csvf.write(requests.get(self.url).content)\n",
    "\n",
    "        # process csv if necessary\n",
    "        dataframe = pd.read_csv(self.merfish_csv)\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"w\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            for colnm, dtype in zip(dataframe.keys()[:9], dataframe.dtypes[:9]):\n",
    "                if dtype.kind == \"O\":\n",
    "                    data = np.require(dataframe[colnm], dtype=\"S36\")\n",
    "                    h5f.create_dataset(colnm, data=data)\n",
    "                else:\n",
    "                    h5f.create_dataset(colnm, data=np.require(dataframe[colnm]))\n",
    "\n",
    "            expression = np.array(dataframe[dataframe.keys()[9:]]).astype(np.float16)\n",
    "            h5f.create_dataset(\"expression\", data=expression)\n",
    "\n",
    "            gene_names = np.array(dataframe.keys()[9:], dtype=\"S80\")\n",
    "            h5f.create_dataset(\"gene_names\", data=gene_names)\n",
    "\n",
    "    def construct_graph(\n",
    "        self, data, anid, breg, n_neighbors, log_transform, neighbor_celltypes, radius\n",
    "    ):\n",
    "        def get_neighbors(edges, x_shape):\n",
    "            return [edges[:, edges[0] == i][1] for i in range(x_shape)]\n",
    "\n",
    "        def get_celltype_simplex(cell_behavior_tensor, neighbors_tensor):\n",
    "            num_classes = cell_behavior_tensor.max() + 1\n",
    "            return torch.cat(\n",
    "                [\n",
    "                    (\n",
    "                        torch.mean(\n",
    "                            1.0\n",
    "                            * F.one_hot(\n",
    "                                cell_behavior_tensor.index_select(0, neighbors),\n",
    "                                num_classes=num_classes,\n",
    "                            ),\n",
    "                            dim=0,\n",
    "                        )\n",
    "                    ).unsqueeze(0)\n",
    "                    for neighbors in neighbors_tensor\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        # get subset of cells in this slice\n",
    "        good = (data.anids == anid) & (data.bregs == breg)\n",
    "\n",
    "        # figure out neighborhood structure\n",
    "        locations_for_this_slice = data.locations[good]\n",
    "\n",
    "        # only include self edges if n_neighbors is 0\n",
    "        if n_neighbors == 0:\n",
    "            edges = np.concatenate(\n",
    "                [\n",
    "                    np.c_[np.array([i]), np.array([i])]\n",
    "                    for i in range(locations_for_this_slice.shape[0])\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            if radius is None:\n",
    "                nbrs = neighbors.NearestNeighbors(\n",
    "                    n_neighbors=n_neighbors + 1, algorithm=\"ball_tree\"\n",
    "                )\n",
    "                nbrs.fit(locations_for_this_slice)\n",
    "                _, kneighbors = nbrs.kneighbors(locations_for_this_slice)\n",
    "                edges = np.concatenate(\n",
    "                    [\n",
    "                        np.c_[kneighbors[:, 0], kneighbors[:, i]]\n",
    "                        for i in range(n_neighbors + 1)\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "\n",
    "                tree = cKDTree(locations_for_this_slice)\n",
    "                kneighbors = tree.query_ball_point(\n",
    "                    locations_for_this_slice, r=radius, return_sorted=False\n",
    "                )\n",
    "                edges = np.concatenate(\n",
    "                    [\n",
    "                        np.c_[\n",
    "                            np.repeat(i, len(kneighbors[i]) - 1),\n",
    "                            [x for x in kneighbors[i] if x != i],\n",
    "                        ]\n",
    "                        for i in range(len(kneighbors))\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        edges = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "        # remove gene 144.  which is bad.  for some reason.\n",
    "        subexpression = data.expression[good]\n",
    "        subexpression = subexpression[:, ~self.bad_genes]\n",
    "\n",
    "        # get behavior ids\n",
    "        behavior_ids = np.array([self.behavior_lookup[x] for x in data.behavior[good]])\n",
    "        celltype_ids = np.array([self.celltype_lookup[x] for x in data.celltypes[good]])\n",
    "        labelinfo = np.c_[behavior_ids, celltype_ids]\n",
    "\n",
    "        # make it into a torch geometric data object, add it to the list!\n",
    "\n",
    "        # if we want to first log transform the data, we do it here\n",
    "        # make this one return statement only changing x\n",
    "        predictors_x = torch.tensor(subexpression.astype(np.float32))\n",
    "        if neighbor_celltypes:\n",
    "            test_simplex = get_celltype_simplex(\n",
    "                torch.tensor(labelinfo[:, 1]),\n",
    "                get_neighbors(edges, predictors_x.shape[0]),\n",
    "            )\n",
    "            predictors_x = torch.cat((predictors_x, test_simplex), dim=1)\n",
    "        if log_transform:\n",
    "            predictors_x = torch.log1p(predictors_x)\n",
    "\n",
    "        return torch_geometric.data.Data(\n",
    "            x=predictors_x,\n",
    "            edge_index=edges,\n",
    "            pos=torch.tensor(locations_for_this_slice.astype(np.float32)),\n",
    "            y=torch.tensor(labelinfo),\n",
    "            bregma=breg,\n",
    "        )\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "    ):\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "            # see if you can update data locations AFTER data was created\n",
    "            # create a deepcopy and then split the locations\n",
    "\n",
    "            # get the (animal_id,bregma) pairs that define a unique slice\n",
    "            unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "            # are we looking at train or test sets?\n",
    "            unique_slices = unique_slices[:150] if train else unique_slices[150:]\n",
    "\n",
    "            # print(len(unique_slices))\n",
    "            axis0_split = np.quantile(data.locations, 0.5, 0)[0]\n",
    "\n",
    "            axis1_split_lower = np.quantile(data.locations[data.locations[:, 0] <= axis0_split][:, 1], 0.5)\n",
    "            axis1_split_higher = np.quantile(data.locations[data.locations[:, 0] >= axis0_split][:, 1], 0.5)\n",
    "\n",
    "\n",
    "            splits = [\n",
    "                np.where((data.locations[:, 0] <= axis0_split) & (data.locations[:, 1] <= axis1_split_lower)),\n",
    "                np.where((data.locations[:, 0] <= axis0_split) & (data.locations[:, 1] >= axis1_split_lower)),\n",
    "                np.where((data.locations[:, 0] >= axis0_split) & (data.locations[:, 1] <= axis1_split_higher)),\n",
    "                np.where((data.locations[:, 0] >= axis0_split) & (data.locations[:, 1] >= axis1_split_higher))\n",
    "            ]\n",
    "\n",
    "            # store all the slices in this list...\n",
    "            data_list = []\n",
    "\n",
    "            for anid, breg in unique_slices:\n",
    "                for split in splits:\n",
    "                    data_split = types.SimpleNamespace(\n",
    "                        anids=h5f[\"Animal_ID\"][split],\n",
    "                        bregs=h5f[\"Bregma\"][split],\n",
    "                        expression=h5f[\"expression\"][split],\n",
    "                        locations=np.c_[h5f[\"Centroid_X\"][split], h5f[\"Centroid_Y\"][split]],\n",
    "                        behavior=h5f[\"Behavior\"][split].astype(\"U\"),\n",
    "                        celltypes=h5f[\"Cell_class\"][split].astype(\"U\"),\n",
    "                    )\n",
    "                    data_list.append(\n",
    "                        self.construct_graph(\n",
    "                            data_split,\n",
    "                            anid,\n",
    "                            breg,\n",
    "                            n_neighbors,\n",
    "                            log_transform,\n",
    "                            neighbor_celltypes,\n",
    "                            radius,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            return data_list\n",
    "\n",
    "\n",
    "class FilteredMerfishDataset(MerfishDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "        animals=None,\n",
    "        bregmas=None,\n",
    "        sexes=None,\n",
    "        behaviors=None,\n",
    "        test_animal=None,\n",
    "        full_animal_holdout=False,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.animals = animals\n",
    "        self.bregmas = bregmas\n",
    "        self.sexes = sexes\n",
    "        self.behaviors = behaviors\n",
    "        self.test_animal = test_animal\n",
    "        self.full_animal_holdout = full_animal_holdout\n",
    "        original_csv_file = super().merfish_csv\n",
    "        new_df = pd.read_csv(original_csv_file)\n",
    "        # print(f\"Original Data {new_df.shape}\")\n",
    "        if self.sexes is not None:\n",
    "            new_df = new_df[new_df[\"Animal_sex\"].isin(self.sexes)]\n",
    "        if self.behaviors is not None:\n",
    "            new_df = new_df[new_df[\"Behavior\"].isin(self.behaviors)]\n",
    "        if self.animals is not None:\n",
    "            new_df = new_df[new_df[\"Animal_ID\"].isin(self.animals)]\n",
    "        if self.bregmas is not None:\n",
    "            new_df = new_df[new_df[\"Bregma\"].isin(self.bregmas)]\n",
    "        if new_df.shape[0] == 0:\n",
    "            raise ValueError(\"Dataframe has no rows. Cannot build graph.\")\n",
    "        new_df.to_csv(str(self.root) + \"/raw/merfish_messi.csv\", index=False)\n",
    "        print(f\"Filtered Data {new_df.shape}\")\n",
    "        # print(\"Filtered csv file created!\")\n",
    "        MerfishDataset.download(self)\n",
    "        super().__init__(\n",
    "            root,\n",
    "            n_neighbors=n_neighbors,\n",
    "            train=train,\n",
    "            log_transform=log_transform,\n",
    "            neighbor_celltypes=neighbor_celltypes,\n",
    "            non_response_genes_file=non_response_genes_file,\n",
    "            radius=radius,\n",
    "        )\n",
    "        # print(\"Filtered hdf5 file created!\")\n",
    "\n",
    "    #     @property\n",
    "    #     def raw_file_names(self):\n",
    "    #         return [\"merfish_messi.csv\", \"merfish_messi.hdf5\"]\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.csv\")\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.hdf5\")\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "    ):\n",
    "        print(self.merfish_hdf5)\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "        anid_to_bregma_count = {\n",
    "            1: 12,\n",
    "            2: 12,\n",
    "            3: 6,\n",
    "            4: 5,\n",
    "            5: 6,\n",
    "            6: 6,\n",
    "            7: 12,\n",
    "            8: 6,\n",
    "            9: 6,\n",
    "            10: 6,\n",
    "            11: 6,\n",
    "            12: 4,\n",
    "            13: 4,\n",
    "            14: 4,\n",
    "            15: 4,\n",
    "            16: 4,\n",
    "            17: 4,\n",
    "            18: 4,\n",
    "            19: 4,\n",
    "            20: 4,\n",
    "            21: 4,\n",
    "            22: 4,\n",
    "            23: 4,\n",
    "            24: 4,\n",
    "            25: 4,\n",
    "            26: 4,\n",
    "            27: 2,\n",
    "            28: 4,\n",
    "            29: 4,\n",
    "            30: 4,\n",
    "        }\n",
    "\n",
    "        # get the (animal_id,bregma) pairs that define a unique slice\n",
    "        unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "        # are we looking at train or test sets?\n",
    "\n",
    "        # if we want a specific animals\n",
    "        if self.test_animal is not None:\n",
    "\n",
    "            animal_path = pathlib.Path(__file__).parent.absolute()\n",
    "            animal_path = animal_path.joinpath(\"animal_id.json\")\n",
    "            with open(animal_path, encoding=\"utf8\") as json_file:\n",
    "                animals = json.load(json_file)\n",
    "\n",
    "            for sex, behavior in it.product(self.sexes, self.behaviors):\n",
    "                try:\n",
    "                    if self.test_animal in animals[behavior][sex]:\n",
    "                        break\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Animal ID {self.test_animal} does not belong\"\n",
    "                    f\"to the set of {self.behaviors}, {self.sexes} animals\"\n",
    "                )\n",
    "\n",
    "            # we need to find which of the slices\n",
    "            sorted_anids = np.sort(np.unique(data.anids))\n",
    "            slices_before_test_anid = 0\n",
    "            for anid in sorted_anids:\n",
    "                if anid != self.test_animal:\n",
    "                    slices_before_test_anid += anid_to_bregma_count[anid]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            mask_train = np.ones(unique_slices.shape[0], dtype=bool)\n",
    "            mask_train[\n",
    "                slices_before_test_anid : (\n",
    "                    slices_before_test_anid + anid_to_bregma_count[self.test_animal]\n",
    "                )\n",
    "            ] = 0\n",
    "            unique_slices = (\n",
    "                unique_slices[(1 - mask_train).astype(\"bool\")]\n",
    "                if not train\n",
    "                else unique_slices[mask_train]\n",
    "            )\n",
    "\n",
    "        elif self.full_animal_holdout and (\n",
    "            len(self.animals) > 1 or np.unique(unique_slices[:, 0]) > 1\n",
    "        ):\n",
    "            print(f\"BEFORE: {unique_slices}\")\n",
    "            min_animal = anid_to_bregma_count[np.min(data.anids)]\n",
    "            unique_slices = (\n",
    "                unique_slices[min_animal:] if train else unique_slices[:min_animal]\n",
    "            )\n",
    "            print(f\"AFTER: {unique_slices}\")\n",
    "\n",
    "        else:\n",
    "            num_slices = len(unique_slices)\n",
    "            print(num_slices)\n",
    "            min_holdout = max(1, round((1 / 7) * num_slices))\n",
    "            unique_slices = (\n",
    "                unique_slices[: (num_slices - min_holdout)]\n",
    "                if train\n",
    "                else unique_slices[(num_slices - min_holdout) :]\n",
    "            )\n",
    "\n",
    "        # store all the slices in this list...\n",
    "        data_list = []\n",
    "        for anid, breg in unique_slices:\n",
    "            data_list.append(\n",
    "                self.construct_graph(\n",
    "                    data,\n",
    "                    anid,\n",
    "                    breg,\n",
    "                    n_neighbors,\n",
    "                    log_transform,\n",
    "                    neighbor_celltypes,\n",
    "                    radius,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f889c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from sklearn import neighbors\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "class MerfishDataset(torch_geometric.data.InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "        splits=2,\n",
    "    ):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # non-response genes (columns) in MERFISH\n",
    "        with open(non_response_genes_file, \"r\", encoding=\"utf8\") as genes_file:\n",
    "            self.features = [int(x) for x in genes_file.read().split(\",\")]\n",
    "            genes_file.close()\n",
    "\n",
    "        # response genes (columns in MERFISH)\n",
    "        self.response_genes = list(set(range(155)) - set(self.features))\n",
    "\n",
    "        data_list = self.construct_graphs(\n",
    "            n_neighbors, train, log_transform, neighbor_celltypes, radius, splits\n",
    "        )\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            self.gene_names = h5f[\"gene_names\"][:][~self.bad_genes].astype(\"U\")\n",
    "\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    # from https://datadryad.org/stash/dataset/doi:10.5061/dryad.8t8s248\n",
    "    url = \"https://datadryad.org/stash/downloads/file_stream/67671\"\n",
    "\n",
    "    behavior_types = [\n",
    "        \"Naive\",\n",
    "        \"Parenting\",\n",
    "        \"Virgin Parenting\",\n",
    "        \"Aggression to pup\",\n",
    "        \"Aggression to adult\",\n",
    "        \"Mating\",\n",
    "    ]\n",
    "    behavior_lookup = {x: i for (i, x) in enumerate(behavior_types)}\n",
    "    cell_types = [\n",
    "        \"Ambiguous\",\n",
    "        \"Astrocyte\",\n",
    "        \"Endothelial 1\",\n",
    "        \"Endothelial 2\",\n",
    "        \"Endothelial 3\",\n",
    "        \"Ependymal\",\n",
    "        \"Excitatory\",\n",
    "        \"Inhibitory\",\n",
    "        \"Microglia\",\n",
    "        \"OD Immature 1\",\n",
    "        \"OD Immature 2\",\n",
    "        \"OD Mature 1\",\n",
    "        \"OD Mature 2\",\n",
    "        \"OD Mature 3\",\n",
    "        \"OD Mature 4\",\n",
    "        \"Pericytes\",\n",
    "    ]\n",
    "    celltype_lookup = {x: i for (i, x) in enumerate(cell_types)}\n",
    "\n",
    "    bad_genes = np.zeros(161, dtype=bool)\n",
    "    bad_genes[[12, 13, 14, 15, 16, 144]] = True\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"merfish.csv\", \"merfish.hdf5\"]\n",
    "\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.csv\")\n",
    "\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.hdf5\")\n",
    "\n",
    "    def download(self):\n",
    "        # download csv if necessary\n",
    "        if not os.path.exists(self.merfish_csv):\n",
    "            with open(self.merfish_csv, \"wb\") as csvf:\n",
    "                csvf.write(requests.get(self.url).content)\n",
    "\n",
    "        # process csv if necessary\n",
    "        dataframe = pd.read_csv(self.merfish_csv)\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"w\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            for colnm, dtype in zip(dataframe.keys()[:9], dataframe.dtypes[:9]):\n",
    "                if dtype.kind == \"O\":\n",
    "                    data = np.require(dataframe[colnm], dtype=\"S36\")\n",
    "                    h5f.create_dataset(colnm, data=data)\n",
    "                else:\n",
    "                    h5f.create_dataset(colnm, data=np.require(dataframe[colnm]))\n",
    "\n",
    "            expression = np.array(dataframe[dataframe.keys()[9:]]).astype(np.float16)\n",
    "            h5f.create_dataset(\"expression\", data=expression)\n",
    "\n",
    "            gene_names = np.array(dataframe.keys()[9:], dtype=\"S80\")\n",
    "            h5f.create_dataset(\"gene_names\", data=gene_names)\n",
    "\n",
    "    # pylint: disable=too-many-statements\n",
    "    def construct_graph(\n",
    "        self,\n",
    "        data,\n",
    "        anid,\n",
    "        breg,\n",
    "        n_neighbors,\n",
    "        log_transform,\n",
    "        neighbor_celltypes,\n",
    "        radius,\n",
    "        splits=1,\n",
    "    ):\n",
    "        def get_neighbors(edges, x_shape):\n",
    "            return [edges[:, edges[0] == i][1] for i in range(x_shape)]\n",
    "\n",
    "        def get_celltype_simplex(cell_behavior_tensor, neighbors_tensor):\n",
    "            num_classes = cell_behavior_tensor.max() + 1\n",
    "            return torch.cat(\n",
    "                [\n",
    "                    (\n",
    "                        torch.mean(\n",
    "                            1.0\n",
    "                            * F.one_hot(\n",
    "                                cell_behavior_tensor.index_select(0, neighbors),\n",
    "                                num_classes=num_classes,\n",
    "                            ),\n",
    "                            dim=0,\n",
    "                        )\n",
    "                    ).unsqueeze(0)\n",
    "                    for neighbors in neighbors_tensor\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        # get subset of cells in this slice\n",
    "        good = (data.anids == anid) & (data.bregs == breg)\n",
    "        \n",
    "        \n",
    "        # index 0 is behavior, index 1 is celltype, # last 2 are the location. Will update it\n",
    "        # to be last 3 in the case that bregma is included in location.\n",
    "        data_for_this_slice = np.concatenate((\n",
    "            data.behavior[good].reshape(-1,1), \n",
    "            data.celltypes[good].reshape(-1,1), \n",
    "            data.expression[good], \n",
    "            data.locations[good]\n",
    "        ), axis=1)\n",
    "        \n",
    "        \n",
    "        # graph splitting\n",
    "        graph_splits = [data_for_this_slice]\n",
    "        new_splits = []\n",
    "        print(splits)\n",
    "        while splits != 0:\n",
    "            for graph in graph_splits:\n",
    "                locations = graph[:, -2:].astype(\"float\")\n",
    "                x_split = np.median(locations, 0)[0]\n",
    "                x_0 = graph[locations[:, 0] < x_split]\n",
    "                x_1 = graph[locations[:, 0] >= x_split]\n",
    "                x_0_locations = x_0[:, -2:].astype(\"float\")\n",
    "                x_1_locations = x_1[:, -2:].astype(\"float\")\n",
    "                y_split_x_0 = np.median(x_0_locations, 0)[1]\n",
    "                y_split_x_1 = np.median(x_1_locations, 0)[1]\n",
    "                x_00 = x_0[x_0_locations[:, 1] < y_split_x_0]\n",
    "                x_01 = x_0[x_0_locations[:, 1] >= y_split_x_0]\n",
    "                x_10 = x_1[x_1_locations[:, 1] < y_split_x_1]\n",
    "                x_11 = x_1[x_1_locations[:, 1] >= y_split_x_1]\n",
    "                new_splits += [x_00, x_01, x_10, x_11]\n",
    "            graph_splits = new_splits\n",
    "            new_splits = []\n",
    "            splits -= 1\n",
    "        print(len(graph_splits))\n",
    "\n",
    "        data_splits = []\n",
    "\n",
    "        for split in graph_splits:\n",
    "            \n",
    "            split_locations = split[:, -2:]\n",
    "\n",
    "            # only include self edges if n_neighbors is 0\n",
    "            if n_neighbors == 0 and radius is None:\n",
    "                edges = np.concatenate(\n",
    "                    [\n",
    "                        np.c_[np.array([i]), np.array([i])]\n",
    "                        for i in range(split_locations.shape[0])\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "\n",
    "                if radius is None:\n",
    "                    nbrs = neighbors.NearestNeighbors(\n",
    "                        n_neighbors=n_neighbors + 1, algorithm=\"ball_tree\"\n",
    "                    )\n",
    "                    nbrs.fit(split_locations)\n",
    "                    _, kneighbors = nbrs.kneighbors(split_locations)\n",
    "                    edges = np.concatenate(\n",
    "                        [\n",
    "                            np.c_[kneighbors[:, 0], kneighbors[:, i]]\n",
    "                            for i in range(n_neighbors + 1)\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "\n",
    "                    tree = cKDTree(split_locations)\n",
    "                    kneighbors = tree.query_ball_point(\n",
    "                        split_locations, r=radius, return_sorted=False\n",
    "                    )\n",
    "                    edges = np.concatenate(\n",
    "                        [\n",
    "                            np.c_[\n",
    "                                np.repeat(i, len(kneighbors[i]) - 1),\n",
    "                                [x for x in kneighbors[i] if x != i],\n",
    "                            ]\n",
    "                            for i in range(len(kneighbors))\n",
    "                        ],\n",
    "                        axis=0,\n",
    "                    )\n",
    "\n",
    "            edges = torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "            # remove gene 144.  which is bad.  for some reason.\n",
    "            subexpression = split[:, 2:-2]\n",
    "            subexpression = subexpression[:, ~self.bad_genes]\n",
    "\n",
    "            # get behavior ids\n",
    "            behavior_ids = np.array(\n",
    "                [self.behavior_lookup[x] for x in split[:, 0]]\n",
    "            )\n",
    "            celltype_ids = np.array(\n",
    "                [self.celltype_lookup[x] for x in split[:, 1]]\n",
    "            )\n",
    "            labelinfo = np.c_[behavior_ids, celltype_ids]\n",
    "\n",
    "            # make it into a torch geometric data object, add it to the list!\n",
    "\n",
    "            # if we want to first log transform the data, we do it here\n",
    "            # make this one return statement only changing x\n",
    "            predictors_x = torch.tensor(subexpression.astype(np.float32))\n",
    "            if neighbor_celltypes:\n",
    "                test_simplex = get_celltype_simplex(\n",
    "                    torch.tensor(labelinfo[:, 1]),\n",
    "                    get_neighbors(edges, predictors_x.shape[0]),\n",
    "                )\n",
    "                predictors_x = torch.cat((predictors_x, test_simplex), dim=1)\n",
    "            if log_transform:\n",
    "                predictors_x = torch.log1p(predictors_x)\n",
    "\n",
    "            data_splits.append(\n",
    "                torch_geometric.data.Data(\n",
    "                    x=predictors_x,\n",
    "                    edge_index=edges,\n",
    "                    pos=torch.tensor(split[:, -2:].astype(\"float\")),\n",
    "                    y=torch.tensor(labelinfo),\n",
    "                    bregma=breg,\n",
    "                    anid=anid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        splits=0,\n",
    "    ):\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "            # see if you can update data locations AFTER data was created\n",
    "            # create a deepcopy and then split the locations\n",
    "\n",
    "            # get the (animal_id,bregma) pairs that define a unique slice\n",
    "            unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "            # are we looking at train or test sets?\n",
    "            # the number for filtering represents animal id\n",
    "            unique_slices = (\n",
    "                unique_slices[unique_slices[:, 0] <= 30]\n",
    "                if train\n",
    "                else unique_slices[unique_slices[:, 0] > 30]\n",
    "            )\n",
    "\n",
    "            # store all the slices in this list...\n",
    "            data_list = []\n",
    "\n",
    "            for anid, breg in unique_slices:\n",
    "                data_list.append(\n",
    "                    self.construct_graph(\n",
    "                        data,\n",
    "                        anid,\n",
    "                        breg,\n",
    "                        n_neighbors,\n",
    "                        log_transform,\n",
    "                        neighbor_celltypes,\n",
    "                        radius,\n",
    "                        splits=splits,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            return sum(data_list, [])\n",
    "\n",
    "\n",
    "class FilteredMerfishDataset(MerfishDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "        animals=None,\n",
    "        bregmas=None,\n",
    "        sexes=None,\n",
    "        behaviors=None,\n",
    "        test_animal=None,\n",
    "        full_animal_holdout=False,\n",
    "        splits=0,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.animals = animals\n",
    "        self.bregmas = bregmas\n",
    "        self.sexes = sexes\n",
    "        self.behaviors = behaviors\n",
    "        self.test_animal = test_animal\n",
    "        self.full_animal_holdout = full_animal_holdout\n",
    "        original_csv_file = super().merfish_csv\n",
    "        new_df = pd.read_csv(original_csv_file)\n",
    "        # print(f\"Original Data {new_df.shape}\")\n",
    "        if self.sexes is not None:\n",
    "            new_df = new_df[new_df[\"Animal_sex\"].isin(self.sexes)]\n",
    "        if self.behaviors is not None:\n",
    "            new_df = new_df[new_df[\"Behavior\"].isin(self.behaviors)]\n",
    "        if self.animals is not None:\n",
    "            new_df = new_df[new_df[\"Animal_ID\"].isin(self.animals)]\n",
    "        if self.bregmas is not None:\n",
    "            new_df = new_df[new_df[\"Bregma\"].isin(self.bregmas)]\n",
    "        if new_df.shape[0] == 0:\n",
    "            raise ValueError(\"Dataframe has no rows. Cannot build graph.\")\n",
    "        new_df.to_csv(str(self.root) + \"/raw/merfish_messi.csv\", index=False)\n",
    "        print(f\"Filtered Data {new_df.shape}\")\n",
    "        # print(\"Filtered csv file created!\")\n",
    "        MerfishDataset.download(self)\n",
    "        super().__init__(\n",
    "            root,\n",
    "            n_neighbors=n_neighbors,\n",
    "            train=train,\n",
    "            log_transform=log_transform,\n",
    "            neighbor_celltypes=neighbor_celltypes,\n",
    "            non_response_genes_file=non_response_genes_file,\n",
    "            radius=radius,\n",
    "            splits=splits,\n",
    "        )\n",
    "        # print(\"Filtered hdf5 file created!\")\n",
    "\n",
    "    #     @property\n",
    "    #     def raw_file_names(self):\n",
    "    #         return [\"merfish_messi.csv\", \"merfish_messi.hdf5\"]\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.csv\")\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.hdf5\")\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        splits=0,\n",
    "    ):\n",
    "        print(self.merfish_hdf5)\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "        anid_to_bregma_count = {\n",
    "            1: 12,\n",
    "            2: 12,\n",
    "            3: 6,\n",
    "            4: 5,\n",
    "            5: 6,\n",
    "            6: 6,\n",
    "            7: 12,\n",
    "            8: 6,\n",
    "            9: 6,\n",
    "            10: 6,\n",
    "            11: 6,\n",
    "            12: 4,\n",
    "            13: 4,\n",
    "            14: 4,\n",
    "            15: 4,\n",
    "            16: 4,\n",
    "            17: 4,\n",
    "            18: 4,\n",
    "            19: 4,\n",
    "            20: 4,\n",
    "            21: 4,\n",
    "            22: 4,\n",
    "            23: 4,\n",
    "            24: 4,\n",
    "            25: 4,\n",
    "            26: 4,\n",
    "            27: 2,\n",
    "            28: 4,\n",
    "            29: 4,\n",
    "            30: 4,\n",
    "        }\n",
    "\n",
    "        # get the (animal_id,bregma) pairs that define a unique slice\n",
    "        unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "        # are we looking at train or test sets?\n",
    "\n",
    "        # if we want a specific animals\n",
    "        if self.test_animal is not None:\n",
    "\n",
    "            # we need to find which of the slices\n",
    "            sorted_anids = np.sort(np.unique(data.anids))\n",
    "            slices_before_test_anid = 0\n",
    "            for anid in sorted_anids:\n",
    "                if anid != self.test_animal:\n",
    "                    slices_before_test_anid += anid_to_bregma_count[anid]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            mask_train = np.ones(unique_slices.shape[0], dtype=bool)\n",
    "            mask_train[\n",
    "                slices_before_test_anid : (\n",
    "                    slices_before_test_anid + anid_to_bregma_count[self.test_animal]\n",
    "                )\n",
    "            ] = 0\n",
    "            unique_slices = (\n",
    "                unique_slices[(1 - mask_train).astype(\"bool\")]\n",
    "                if not train\n",
    "                else unique_slices[mask_train]\n",
    "            )\n",
    "\n",
    "        elif self.full_animal_holdout and (\n",
    "            len(self.animals) > 1 or np.unique(unique_slices[:, 0]) > 1\n",
    "        ):\n",
    "            print(f\"BEFORE: {unique_slices}\")\n",
    "            min_animal = anid_to_bregma_count[np.min(data.anids)]\n",
    "            unique_slices = (\n",
    "                unique_slices[min_animal:] if train else unique_slices[:min_animal]\n",
    "            )\n",
    "            print(f\"AFTER: {unique_slices}\")\n",
    "\n",
    "        else:\n",
    "            num_slices = len(unique_slices)\n",
    "            print(num_slices)\n",
    "            min_holdout = max(1, round((1 / 7) * num_slices))\n",
    "            unique_slices = (\n",
    "                unique_slices[: (num_slices - min_holdout)]\n",
    "                if train\n",
    "                else unique_slices[(num_slices - min_holdout) :]\n",
    "            )\n",
    "\n",
    "        # store all the slices in this list...\n",
    "        data_list = []\n",
    "        for anid, breg in unique_slices:\n",
    "            data_list.append(\n",
    "                self.construct_graph(\n",
    "                    data,\n",
    "                    anid,\n",
    "                    breg,\n",
    "                    n_neighbors,\n",
    "                    log_transform,\n",
    "                    neighbor_celltypes,\n",
    "                    radius,\n",
    "                    splits,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(data_list, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f62af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n",
      "2\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "test = MerfishDataset(\"../data\", radius=16, splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d64919c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test) # should yield 2400. Since vanilla merfish was 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b7c52c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.max(\n",
       " values=tensor([ 2243.3716, -2335.5574]),\n",
       " indices=tensor([6493, 4778])),\n",
       " torch.return_types.min(\n",
       " values=tensor([  451.1422, -4127.1948]),\n",
       " indices=tensor([6051, 6480])))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(test[0].pos, axis=0), torch.min(test[0].pos, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b06cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = test[12].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1d32ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1693.8762207 , -2281.63061523])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.quantile(example, 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69f8dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis0_split = np.quantile(example, 0.5, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d3fc00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3025.6965, -3127.4504, -3110.4485,  ..., -1669.1136, -1805.7301,\n",
       "        -1468.3505])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[example[:, 0] <= axis0_split][:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8caed005",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis1_split_lower = np.quantile(example[example[:, 0] <= axis0_split][:, 1], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89814171",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis1_split_higher = np.quantile(example[example[:, 0] >= axis0_split][:, 1], 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c59b7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2277.1319580078125, -2285.065673828125)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis1_split_lower, axis1_split_higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "38fd766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ True,  True,  True,  ..., False, False, False])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(example[:, 0] <= axis0_split) & (example[:, 1] <= axis1_split_lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8c8ade62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 7],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        ...,\n",
       "        [0, 7],\n",
       "        [0, 1],\n",
       "        [0, 1]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[12].y[[(example[:, 0] <= axis0_split) & (example[:, 1] <= axis1_split_lower)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28dde24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
