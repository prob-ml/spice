{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183c4561",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae38491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg\n",
    "rng=np.random.default_rng()\n",
    "import tqdm.notebook\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import ipywidgets\n",
    "import sklearn.neighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import requests\n",
    "\n",
    "from spatial.merfish_dataset import FilteredMerfishDataset, MerfishDataset\n",
    "from spatial.models.monet_ae import MonetAutoencoder2D, TrivialAutoencoder\n",
    "from spatial.train import train\n",
    "from spatial.predict import test\n",
    "\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import hydra\n",
    "from hydra.experimental import compose, initialize\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from MESSI_for_reproduction.tutorials.context import messi\n",
    "from messi.data_processing import *\n",
    "from messi.hme import hme\n",
    "from messi.gridSearch import gridSearch\n",
    "\n",
    "original_url= \"https://datadryad.org/stash/downloads/file_stream/67671\"\n",
    "csv_location='../data/spatial/moffit_merfish/original_file.csv'\n",
    "h5ad_location='../data/spatial/moffit_merfish/original_file.h5ad'\n",
    "connectivity_matrix_template='../data/spatial/moffit_merfish/connectivity_%dneighbors.h5ad'\n",
    "genetypes_location='/data/spatial/moffit_merfish/genetypes.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8531233",
   "metadata": {},
   "source": [
    "# LightGBM Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a73f00",
   "metadata": {},
   "source": [
    "###### download csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "with open(csv_location, \"wb\") as csvf:\n",
    "    csvf.write(requests.get(original_url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747770a4",
   "metadata": {},
   "source": [
    "##### munge into hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(csv_location)\n",
    "\n",
    "dct={}\n",
    "for colnm, dtype in zip(dataframe.keys()[:9], dataframe.dtypes[:9]):\n",
    "    if dtype.kind == \"O\":\n",
    "        dct[colnm]=np.require(dataframe[colnm], dtype=\"U36\")\n",
    "    else:\n",
    "        dct[colnm]=np.require(dataframe[colnm])\n",
    "expression = np.array(dataframe[dataframe.keys()[9:]]).astype(np.float16)\n",
    "gene_names = np.array(dataframe.keys()[9:], dtype=\"U80\")\n",
    "cellid=dct.pop('Cell_ID')\n",
    "\n",
    "ad=anndata.AnnData(\n",
    "    X=expression,\n",
    "    var=pd.DataFrame(index=gene_names),\n",
    "    obs=pd.DataFrame(dct,index=cellid)\n",
    ")\n",
    "\n",
    "ad.write_h5ad(h5ad_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934bded8",
   "metadata": {},
   "source": [
    "##### supplement hdf5 file with a column indicating \"tissue id\" for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7110937",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "animal_ids=np.unique(ad.obs['Animal_ID'])\n",
    "bregmas=np.unique(ad.obs['Bregma'])\n",
    "tissue_id=np.zeros(len(ad),dtype=int)\n",
    "n_tissues=0\n",
    "    \n",
    "for aid in animal_ids:\n",
    "    for bregma in bregmas:\n",
    "        good=(ad.obs['Animal_ID']==aid)&(ad.obs['Bregma']==bregma)\n",
    "        if np.sum(good)>0:\n",
    "            tissue_id[good]=n_tissues\n",
    "            n_tissues+=1\n",
    "ad.obs['Tissue_ID']=tissue_id\n",
    "ad.write_h5ad(h5ad_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab22908",
   "metadata": {},
   "source": [
    "##### create global graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "row=np.zeros(0,dtype=int)\n",
    "col=np.zeros(0,dtype=int)\n",
    "radius=70\n",
    "mode=\"rad\"\n",
    "\n",
    "for tid in tqdm.notebook.tqdm(np.unique(ad.obs['Tissue_ID'])):\n",
    "    good=ad.obs['Tissue_ID']==tid\n",
    "    pos=np.array(ad.obs[good][['Centroid_X','Centroid_Y']])\n",
    "    if mode == \"neighbors\":\n",
    "        if nneigh == 0:\n",
    "            E = csr_matrix(np.eye(pos.shape[0]))\n",
    "        else:\n",
    "            p=sklearn.neighbors.BallTree(pos)\n",
    "            E=sklearn.neighbors.kneighbors_graph(pos,nneigh,mode='connectivity')\n",
    "        col=np.r_[col,idxs[E.tocoo().col]]\n",
    "        row=np.r_[row,idxs[E.tocoo().row]]\n",
    "    if mode == \"rad\":\n",
    "        p=sklearn.spatial.cKDTree(pos)\n",
    "        E=p.query_ball_point(pos, r=radius, return_sorted=False)\n",
    "    idxs=np.where(good)[0]\n",
    "    \n",
    "E=sp.sparse.coo_matrix((np.ones(len(col)),(row,col)),shape=(len(ad),len(ad))).tocsr()\n",
    "if mode == \"neighbors\":\n",
    "    anndata.AnnData(E).write_h5ad(connectivity_matrix_template%nneigh)\n",
    "if mode == \"rad\":\n",
    "    anndata.AnnData(E).write_h5ad(connectivity_matrix_template%radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb479e96",
   "metadata": {},
   "source": [
    "##### write down ligand/receptor sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37433743",
   "metadata": {},
   "outputs": [],
   "source": [
    "ligands=np.array(['Cbln1', 'Cxcl14', 'Cbln2', 'Vgf', 'Scg2', 'Cartpt', 'Tac2',\n",
    "       'Bdnf', 'Bmp7', 'Cyr61', 'Fn1', 'Fst', 'Gad1', 'Ntng1', 'Pnoc',\n",
    "       'Selplg', 'Sema3c', 'Sema4d', 'Serpine1', 'Adcyap1', 'Cck', 'Crh',\n",
    "       'Gal', 'Gnrh1', 'Nts', 'Oxt', 'Penk', 'Sst', 'Tac1', 'Trh', 'Ucn3'])\n",
    "\n",
    "receptors=np.array(['Crhbp', 'Gabra1', 'Gpr165', 'Glra3', 'Gabrg1', 'Adora2a',\n",
    "       'Avpr1a', 'Avpr2', 'Brs3', 'Calcr', 'Cckar', 'Cckbr', 'Crhr1',\n",
    "       'Crhr2', 'Galr1', 'Galr2', 'Grpr', 'Htr2c', 'Igf1r', 'Igf2r',\n",
    "       'Kiss1r', 'Lepr', 'Lpar1', 'Mc4r', 'Npy1r', 'Npy2r', 'Ntsr1',\n",
    "       'Oprd1', 'Oprk1', 'Oprl1', 'Oxtr', 'Pdgfra', 'Prlr', 'Ramp3',\n",
    "       'Rxfp1', 'Slc17a7', 'Slc18a2', 'Tacr1', 'Tacr3', 'Trhr'])\n",
    "\n",
    "response_genes=np.array(['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Fos', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2'])\n",
    "cell_types = [\n",
    "        \"Ambiguous\",\n",
    "        \"Astrocyte\",\n",
    "        \"Endothelial 1\",\n",
    "        \"Endothelial 2\",\n",
    "        \"Endothelial 3\",\n",
    "        \"Ependymal\",\n",
    "        \"Excitatory\",\n",
    "        \"Inhibitory\",\n",
    "        \"Microglia\",\n",
    "        \"OD Immature 1\",\n",
    "        \"OD Immature 2\",\n",
    "        \"OD Mature 1\",\n",
    "        \"OD Mature 2\",\n",
    "        \"OD Mature 3\",\n",
    "        \"OD Mature 4\",\n",
    "        \"Pericytes\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9516fb1",
   "metadata": {},
   "source": [
    "##### run a simple experiment: use ligands and receptors to predict response genes in excitatory cells, with a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "nneigh=30\n",
    "radius=70\n",
    "mode=\"rad\"\n",
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "if mode == \"neighbors\":\n",
    "    connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%nneigh).X\n",
    "if mode == \"rad\":\n",
    "     connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%radius).X\n",
    "gene_lookup={x:i for (i,x) in enumerate(ad.var.index)}\n",
    "\n",
    "with open(genetypes_location,'rb') as f:\n",
    "    genetypes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9044d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encode cell classes\n",
    "def oh_encode(lst):\n",
    "    lst=np.array(lst)\n",
    "    group_names=np.unique(lst)\n",
    "    group_indexes=np.zeros((len(lst),len(group_names)),dtype=bool)\n",
    "    for i,nm in enumerate(group_names):\n",
    "        group_indexes[lst==nm,i]=True\n",
    "    return group_names,group_indexes\n",
    "cell_classes,cell_class_onehots=oh_encode(ad.obs['Cell_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aca7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to construct a prediction problem for a subset of cells\n",
    "\n",
    "def construct_problem(mask,target_gene,neighbor_genes,self_genes,filter_excitatory=False):\n",
    "    '''\n",
    "    mask -- set of cells\n",
    "    target_gene -- gene to predict\n",
    "    neighbor_genes -- names of genes which will be read from neighbors\n",
    "    self_genes -- names of genes which will be read from target cell\n",
    "    '''\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # load subset of data relevant to mask\n",
    "    local_processed_expression=np.log1p(ad.X[mask].astype(float)) # get expression on subset of cells\n",
    "    local_edges=connectivity_matrix[mask][:,mask]   # get edges for subset\n",
    "    \n",
    "    selfset_idxs=[gene_lookup[x] for x in self_genes] # collect the column indexes associated with them\n",
    "    selfset_exprs = local_processed_expression[:,selfset_idxs] # collect ligand and receptor expressions\n",
    "    \n",
    "    feature_names += [x for x in self_genes]\n",
    "    \n",
    "    neighborset_idxs=[gene_lookup[x] for x in neighbor_genes] # collect the column indexes associated with them\n",
    "    neighset_exprs = local_processed_expression[:,neighborset_idxs] # collect ligand and receptor expressions\n",
    "    \n",
    "    feature_names += [x + \" from Neighbors\" for x in neighbor_genes]\n",
    "    \n",
    "    n_neighs=(local_edges@np.ones(local_edges.shape[0]))\n",
    "    print(n_neighs)\n",
    "    neigh_avgs = (local_edges@neighset_exprs) / n_neighs[:,None] # average ligand/receptor for neighbors\n",
    "    \n",
    "    neigh_cellclass_avgs = (local_edges@cell_class_onehots[mask]) / n_neighs[:,None] # celltype simplex\n",
    "    \n",
    "    feature_names += [f\"Cell Class {cell_types[x]}\" for x in range(16)]\n",
    "    \n",
    "    positions=np.array(ad.obs[['Centroid_X','Centroid_Y','Bregma']])[mask] # get positions\n",
    "    \n",
    "    feature_names += ['Centroid_X','Centroid_Y','Bregma']\n",
    "    \n",
    "    covariates=np.c_[selfset_exprs,neigh_avgs,neigh_cellclass_avgs,positions] # collect all covariates\n",
    "    predict = local_processed_expression[:,gene_lookup[target_gene]] # collect what we're supposed to predict\n",
    "    \n",
    "    print(selfset_exprs.shape, neigh_avgs.shape, neigh_cellclass_avgs.shape, positions.shape)\n",
    "    \n",
    "    if filter_excitatory:\n",
    "    \n",
    "        excites=(ad.obs['Cell_class']=='Excitatory')[mask] # get the subset of these cells which are excitatory\n",
    "        covariates=covariates[excites] # subset to excites\n",
    "        predict=predict[excites]       # subset to excites\n",
    "    \n",
    "    return covariates,predict,feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_genes=['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2']\n",
    "\n",
    "import time\n",
    "import json\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "all_MAEs = []\n",
    "\n",
    "time_dict = {}\n",
    "L1_loss_dict = {}\n",
    "\n",
    "for animal in [1,2,3,4]:\n",
    "    start = time.time()\n",
    "    MAE_list = []\n",
    "    for target_gene in response_genes:\n",
    "        neighset=genetypes['ligands']\n",
    "        oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "        # oset=neighset\n",
    "\n",
    "        # oset=[]\n",
    "        # neighset=[]\n",
    "        \n",
    "        train_animals = [1,2,3,4]\n",
    "        train_animals.remove(animal)\n",
    "        print(train_animals)\n",
    "        # FIX THIS SO THAT ONLY FIRST 4 ANIMALS GET USED\n",
    "        trainX,trainY=construct_problem((ad.obs['Animal_ID']!=animal)&(ad.obs['Animal_ID']<=4),target_gene,neighset,oset,True)\n",
    "        testX,testY=construct_problem((ad.obs['Animal_ID']==animal),target_gene,neighset,oset,True)\n",
    "\n",
    "        print(trainX.shape,trainY.shape)\n",
    "        print(testX.shape,testY.shape)\n",
    "\n",
    "        # whiten covariates\n",
    "        mu=np.mean(trainX,axis=0)\n",
    "        sig=np.std(trainX,axis=0)\n",
    "        trainX=(trainX-mu)/sig\n",
    "        testX=(testX-mu)/sig\n",
    "\n",
    "        model=HistGradientBoostingRegressor(loss=\"absolute_error\")\n",
    "        model.fit(trainX,trainY)\n",
    "        MAE_list.append(np.mean(np.abs(model.predict(testX)-testY)))\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    all_MAEs.append(np.mean(MAE_list))\n",
    "    \n",
    "print(np.mean(all_MAEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d595d",
   "metadata": {},
   "source": [
    "CV w/ Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce33c89",
   "metadata": {},
   "source": [
    "# MESSI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MESSI(sex, behavior, celltype, train_animals):\n",
    "    input_path = 'input/'\n",
    "    output_path = 'output/'\n",
    "    data_type = 'merfish'\n",
    "    sex = sex\n",
    "    behavior = behavior\n",
    "    behavior_no_space = behavior.replace(\" \", \"_\")\n",
    "    current_cell_type = celltype\n",
    "    current_cell_type_no_space = current_cell_type.replace(\" \", \"_\")\n",
    "\n",
    "    grid_search = False\n",
    "    n_sets = 5  # for example usage only; we recommend 5\n",
    "\n",
    "    n_classes_0 = 1\n",
    "    n_epochs = 20  # for example usage only; we recommend using the default 20 n_epochs \n",
    "    \n",
    "    read_in_functions = {'merfish': [read_meta_merfish, read_merfish_data, get_idx_per_dataset_merfish],\n",
    "                'merfish_cell_line': [read_meta_merfish_cell_line, read_merfish_cell_line_data, get_idx_per_dataset_merfish_cell_line],\n",
    "                'starmap': [read_meta_starmap_combinatorial, read_starmap_combinatorial, get_idx_per_dataset_starmap_combinatorial]}\n",
    "\n",
    "    # set data reading functions corresponding to the data type\n",
    "    if data_type in ['merfish', 'merfish_cell_line', 'starmap']:\n",
    "        read_meta = read_in_functions[data_type][0]\n",
    "        read_data = read_in_functions[data_type][1]\n",
    "        get_idx_per_dataset = read_in_functions[data_type][2]\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Now only support processing 'merfish', 'merfish_cell_line' or 'starmap'\")\n",
    "\n",
    "    # read in ligand and receptor lists\n",
    "    l_u, r_u = get_lr_pairs(input_path='input/')  # may need to change to the default value\n",
    "\n",
    "    # read in meta information about the dataset\n",
    "    meta_all, meta_all_columns, cell_types_dict, genes_list, genes_list_u, \\\n",
    "    response_list_prior, regulator_list_prior = \\\n",
    "        read_meta(input_path, behavior, sex, l_u, r_u)  # TO BE MODIFIED: number of responses\n",
    "\n",
    "    # get all available animals/samples\n",
    "    all_animals = list(set(meta_all[:, meta_all_columns['Animal_ID']]))\n",
    "    print(all_animals)\n",
    "    \n",
    "    test_animals  = [np.max(all_animals)]\n",
    "    samples_test = np.array(test_animals)\n",
    "    samples_train = train_animals\n",
    "    print(f\"Test set is {samples_test}\")\n",
    "    print(f\"Training set is {samples_train}\")\n",
    "    \n",
    "    n_experts_types = {'Inhibitory': {1: 10, 2: 10, 3: 10, 4: 10}, \n",
    "                   'Excitatory': {1: 8, 2: 8, 3: 10, 4: 10},\n",
    "                   'Astrocyte': {1: 4, 2: 4, 3: 3, 4: 3},\n",
    "                   'OD Mature 2' : {1: 3, 2: 3, 3: 4, 4: 3},\n",
    "                   'Endothelial 1': {1: 1, 2: 1, 3: 2, 4: 2},\n",
    "                   'OD Immature 1': {1: 1, 2: 1, 3: 2, 4: 2},\n",
    "                   'OD Mature 1': {1: 1, 2: 1, 3: 1, 4: 1},\n",
    "                   'Microglia': {1: 1, 2: 1, 3: 1, 4: 1}}\n",
    "    \n",
    "    n_classes_1 = n_experts_types[\"Excitatory\"][test_animals[0]]\n",
    "\n",
    "    preprocess = 'neighbor_cat'\n",
    "    top_k_response = None  # for example usage only; we recommend use all responses (i.e. None)\n",
    "    top_k_regulator = None\n",
    "    response_type = 'original'  # use raw values to fit the model\n",
    "    condition = f\"response_{top_k_response}_l1_{n_classes_0}_l2_{n_classes_1}\"\n",
    "\n",
    "    if grid_search:\n",
    "        condition = f\"response_{top_k_response}_l1_{n_classes_0}_l2_grid_search\"\n",
    "    else:\n",
    "        condition = f\"response_{top_k_response}_l1_{n_classes_0}_l2_{n_classes_1}\"\n",
    "    \n",
    "    bregma = None\n",
    "    idx_train, idx_test, idx_train_in_general, \\\n",
    "    idx_test_in_general, idx_train_in_dataset, \\\n",
    "    idx_test_in_dataset, meta_per_dataset_train, \\\n",
    "    meta_per_dataset_test = find_idx_for_train_test(samples_train, samples_test, \n",
    "                                                    meta_all, meta_all_columns, data_type, \n",
    "                                                    current_cell_type, get_idx_per_dataset,\n",
    "                                                    return_in_general = False, \n",
    "                                                    bregma=bregma)\n",
    "    \n",
    "    data_sets = []\n",
    "\n",
    "    for animal_id, bregma in meta_per_dataset_train:\n",
    "        hp, hp_cor, hp_genes = read_data(input_path, bregma, animal_id, genes_list, genes_list_u)\n",
    "\n",
    "        if hp is not None:\n",
    "            hp_columns = dict(zip(hp.columns, range(0, len(hp.columns))))\n",
    "            hp_np = hp.to_numpy()\n",
    "        else:\n",
    "            hp_columns = None\n",
    "            hp_np = None\n",
    "        hp_cor_columns = dict(zip(hp_cor.columns, range(0, len(hp_cor.columns))))\n",
    "        hp_genes_columns = dict(zip(hp_genes.columns, range(0, len(hp_genes.columns))))\n",
    "        data_sets.append([hp_np, hp_columns, hp_cor.to_numpy(), hp_cor_columns,\n",
    "                          hp_genes.to_numpy(), hp_genes_columns])\n",
    "        del hp, hp_cor, hp_genes\n",
    "\n",
    "    datasets_train = data_sets\n",
    "\n",
    "    data_sets = []\n",
    "\n",
    "    for animal_id, bregma in meta_per_dataset_test:\n",
    "        hp, hp_cor, hp_genes = read_data(input_path, bregma, animal_id, genes_list, genes_list_u)\n",
    "\n",
    "        if hp is not None:\n",
    "            hp_columns = dict(zip(hp.columns, range(0, len(hp.columns))))\n",
    "            hp_np = hp.to_numpy()\n",
    "        else:\n",
    "            hp_columns = None\n",
    "            hp_np = None\n",
    "\n",
    "        hp_cor_columns = dict(zip(hp_cor.columns, range(0, len(hp_cor.columns))))\n",
    "        hp_genes_columns = dict(zip(hp_genes.columns, range(0, len(hp_genes.columns))))\n",
    "        data_sets.append([hp_np, hp_columns, hp_cor.to_numpy(), hp_cor_columns,\n",
    "                          hp_genes.to_numpy(), hp_genes_columns])\n",
    "        del hp, hp_cor, hp_genes\n",
    "\n",
    "    datasets_test = data_sets\n",
    "\n",
    "    del data_sets\n",
    "    \n",
    "    if data_type == 'merfish_rna_seq':\n",
    "        neighbors_train = None\n",
    "        neighbors_test = None\n",
    "    else: \n",
    "        if data_type == 'merfish':\n",
    "            dis_filter = 100\n",
    "        else:\n",
    "            dis_filter = 1e9  \n",
    "\n",
    "        neighbors_train = get_neighbors_datasets(datasets_train, \"Del\", k=10, dis_filter=dis_filter, include_self = False)\n",
    "        neighbors_test = get_neighbors_datasets(datasets_test, \"Del\", k=10, dis_filter=dis_filter, include_self = False)\n",
    "        \n",
    "    lig_n =  {'name':'regulators_neighbor','helper':preprocess_X_neighbor_per_cell, \n",
    "                      'feature_list_type': 'regulator_neighbor', 'per_cell':True, 'baseline':False, \n",
    "                      'standardize': True, 'log':True, 'poly':False}\n",
    "    rec_s = {'name':'regulators_self','helper':preprocess_X_self_per_cell, \n",
    "                          'feature_list_type': 'regulator_self', 'per_cell':True, 'baseline':False, \n",
    "                          'standardize': True, 'log':True, 'poly':False}\n",
    "    lig_s = {'name':'regulators_neighbor_self','helper':preprocess_X_self_per_cell, \n",
    "                          'feature_list_type':'regulator_neighbor', 'per_cell':True, 'baseline':False, \n",
    "                          'standardize': True, 'log':True, 'poly':False}\n",
    "    type_n =  {'name': 'neighbor_type','helper':preprocess_X_neighbor_type_per_dataset, \n",
    "                          'feature_list_type':None,'per_cell':False, 'baseline':False, \n",
    "                          'standardize': True, 'log':False, 'poly':False}\n",
    "    base_s = {'name':'baseline','helper':preprocess_X_baseline_per_dataset,'feature_list_type':None, \n",
    "                          'per_cell':False, 'baseline':True, 'standardize': True, 'log':False, 'poly':False}\n",
    "    \n",
    "    if data_type == 'merfish_cell_line':\n",
    "        feature_types = [lig_n, rec_s, base_s, lig_s]\n",
    "\n",
    "    else:\n",
    "        feature_types = [lig_n, rec_s, type_n , base_s, lig_s]\n",
    "    \n",
    "    X_trains, X_tests, regulator_list_neighbor, regulator_list_self  = prepare_features(data_type, datasets_train, datasets_test, meta_per_dataset_train, meta_per_dataset_test, \n",
    "                     idx_train, idx_test, idx_train_in_dataset, idx_test_in_dataset,neighbors_train, neighbors_test,\n",
    "                    feature_types, regulator_list_prior, top_k_regulator, \n",
    "                     genes_list_u, l_u, r_u,cell_types_dict)\n",
    "    \n",
    "    total_regulators = regulator_list_neighbor + regulator_list_self\n",
    "    \n",
    "    log_response = True  # take log transformation of the response genes\n",
    "    \n",
    "    Y_train, Y_train_true, Y_test, Y_test_true, response_list = prepare_responses(data_type, datasets_train,\n",
    "                                                                                  datasets_test, idx_train_in_general,\n",
    "                                                                                  idx_test_in_general,\n",
    "                                                                                  idx_train_in_dataset,\n",
    "                                                                                  idx_test_in_dataset, neighbors_train,\n",
    "                                                                                  neighbors_test,\n",
    "                                                                                  response_type, log_response,\n",
    "                                                                                  response_list_prior, top_k_response,\n",
    "                                                                                  genes_list_u, l_u, r_u)\n",
    "\n",
    "    if grid_search:\n",
    "        X_trains_gs = copy.deepcopy(X_trains)\n",
    "        Y_train_gs = copy.copy(Y_train)\n",
    "\n",
    "    ### Transform and combine different type of features\n",
    "\n",
    "    # transform features\n",
    "    transform_features(X_trains, X_tests, feature_types)\n",
    "    print(f\"Minimum value after transformation can below 0: {np.min(X_trains['regulators_self'])}\")\n",
    "\n",
    "    if data_type == 'merfish':\n",
    "        num_coordinates = 3\n",
    "    elif data_type == 'starmap' or data_type == 'merfish_cell_line':\n",
    "        num_coordinates = 2\n",
    "    else:\n",
    "        num_coordinates = None\n",
    "\n",
    "    if np.ndim(X_trains['baseline']) > 1 and np.ndim(X_tests['baseline']) > 1:\n",
    "        X_train, X_train_clf_1, X_train_clf_2 = combine_features(X_trains, preprocess, num_coordinates)\n",
    "        X_test, X_test_clf_1, X_test_clf_2 = combine_features(X_tests, preprocess, num_coordinates)\n",
    "    elif np.ndim(X_trains['baseline']) > 1:\n",
    "        X_train, X_train_clf_1, X_train_clf_2 = combine_features(X_trains, preprocess, num_coordinates)\n",
    "\n",
    "    print(f\"Dimension of X train is: {X_train.shape}\")\n",
    "    print(f\"Dimension of Y train is: {Y_train.shape}\")\n",
    "\n",
    "    ## Construct and train MESSI model\n",
    "\n",
    "    ### set default parameters\n",
    "\n",
    "    # ------ set parameters ------\n",
    "    model_name_gates = 'logistic'\n",
    "    model_name_experts = 'mrots'\n",
    "    num_response = Y_train.shape[1]\n",
    "\n",
    "    # default values \n",
    "    soft_weights = True\n",
    "    partial_fit_expert = True\n",
    "\n",
    "    # specify default parameters for MESSI\n",
    "    model_params = {'n_classes_0': n_classes_0,\n",
    "                    'n_classes_1': n_classes_1,\n",
    "                    'model_name_gates': model_name_gates,\n",
    "                    'model_name_experts': model_name_experts,\n",
    "                    'num_responses': Y_train.shape[1],\n",
    "                    'soft_weights': soft_weights,\n",
    "                    'partial_fit_expert': partial_fit_expert,\n",
    "                    'n_epochs': n_epochs,\n",
    "                    'tolerance': 3}\n",
    "\n",
    "    ### set up directory to save results\n",
    "\n",
    "    # set up directory for saving the model\n",
    "    sub_condition = f\"{condition}_{model_name_gates}_{model_name_experts}\"\n",
    "    sub_dir = f\"{data_type}/{behavior_no_space}/{sex}/{current_cell_type_no_space}/{preprocess}/{sub_condition}\"\n",
    "    current_dir = os.path.join(output_path, sub_dir)\n",
    "\n",
    "    if not os.path.exists(current_dir):\n",
    "        os.makedirs(current_dir)\n",
    "\n",
    "    print(f\"Model and validation results (if appliable) saved to: {current_dir}\")\n",
    "\n",
    "    suffix = f\"_{test_animals}\"\n",
    "\n",
    "    ### conduct grid seach for hyper-parameters if needed \n",
    "\n",
    "    # search range for number of experts; for example usage only, we recommend 4\n",
    "    search_range_dict = {'Excitatory': range(4,5), 'U-2_OS': range(1,3), \\\n",
    "                            'STARmap_excitatory': range(1,3)}  \n",
    "\n",
    "\n",
    "    if grid_search:\n",
    "        # prepare input meta data\n",
    "        if data_type == 'merfish':\n",
    "            meta_per_part = [tuple(i) for i in meta_per_dataset_train]\n",
    "            meta_idx = meta2idx(idx_train_in_dataset, meta_per_part)\n",
    "        else:\n",
    "            meta_per_part, meta_idx = combineParts(samples_train, datasets_train, idx_train_in_dataset)\n",
    "\n",
    "        # prepare parameters list to be tuned\n",
    "        if data_type == 'merfish_cell_line':\n",
    "            current_cell_type_data = 'U-2_OS'\n",
    "        elif data_type == 'starmap':\n",
    "            current_cell_type_data = 'STARmap_excitatory'\n",
    "        else:\n",
    "            current_cell_type_data = \"Excitatory\"\n",
    "            \n",
    "\n",
    "        params = {'n_classes_1': list(search_range_dict[current_cell_type_data]), 'soft_weights': [True, False],\n",
    "                  'partial_fit_expert': [True, False]}\n",
    "\n",
    "        keys, values = zip(*params.items())\n",
    "        params_list = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        new_params_list = []\n",
    "        for d in params_list:\n",
    "            if d['n_classes_1'] == 1:\n",
    "                if d['soft_weights'] and d['partial_fit_expert']:\n",
    "                    # n_expert = 1, soft or hard are equivalent\n",
    "                    new_params_list.append(d)\n",
    "            else:\n",
    "                if d['soft_weights'] == d['partial_fit_expert']:\n",
    "                    new_params_list.append(d)\n",
    "        ratio = 0.2\n",
    "\n",
    "        # initialize with default values\n",
    "        model_params_val = model_params.copy()\n",
    "        model_params_val['n_epochs'] = 1  # increase for validation models to converge\n",
    "        model_params_val['tolerance'] = 0\n",
    "        print(f\"Default model parameters for validation {model_params_val}\")\n",
    "        model = hme(**model_params_val)\n",
    "\n",
    "        gs = gridSearch(params, model, ratio, n_sets, new_params_list)\n",
    "        gs.generate_val_sets(samples_train, meta_per_part)\n",
    "        gs.runCV(X_trains_gs, Y_train_gs, meta_per_part, meta_idx, feature_types, data_type,\n",
    "                 preprocess)\n",
    "        gs.get_best_parameter()\n",
    "        print(f\"Best params from grid search: {gs.best_params}\")\n",
    "\n",
    "        # modify the parameter setting\n",
    "        for key, value in gs.best_params.items():\n",
    "            model_params[key] = value\n",
    "\n",
    "        print(f\"Model parameters for training after grid search {model_params}\")\n",
    "\n",
    "        filename = f\"validation_results{suffix}.pickle\"\n",
    "        pickle.dump(gs, open(os.path.join(current_dir, filename), 'wb'))\n",
    "\n",
    "    ### fit the full data with specified/selected hyperparameter  \n",
    "\n",
    "    if grid_search and 'n_classes_1' in params:\n",
    "        model = AgglomerativeClustering(n_clusters=gs.best_params['n_classes_1'])\n",
    "    else:\n",
    "        model = AgglomerativeClustering(n_classes_1)\n",
    "\n",
    "    model = model.fit(Y_train)\n",
    "    hier_labels = [model.labels_]\n",
    "    model_params['init_labels_1'] = hier_labels\n",
    "\n",
    "    # ------ construct MESSI  ------\n",
    "    model = hme(**model_params)\n",
    "    # train\n",
    "    model.train(X_train, X_train_clf_1, X_train_clf_2, Y_train)\n",
    "\n",
    "    ### save the model\n",
    "\n",
    "    filename = f\"hme_model{suffix}.pickle\"\n",
    "\n",
    "    pickle.dump(model, open(os.path.join(current_dir, filename), 'wb'))\n",
    "\n",
    "    ## Make predictions on the test data\n",
    "\n",
    "    ### load the saved model\n",
    "\n",
    "    saved_model = pickle.load(open(os.path.join(current_dir, filename), 'rb'))\n",
    "\n",
    "    ### make predictions\n",
    "\n",
    "    Y_hat_final = saved_model.predict(X_test, X_test_clf_1, X_test_clf_2)\n",
    "    print(f\"Mean absolute value : {(abs(Y_test - Y_hat_final).mean(axis=1)).mean()}\")\n",
    "    print(f\"{sex}_{behavior}_{celltype} inference completed!\")\n",
    "    return (abs(Y_test - Y_hat_final).mean(axis=1)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23a90c",
   "metadata": {},
   "source": [
    "# XGBoost Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959cdfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in merfish dataset and get columns names\n",
    "import pandas as pd\n",
    "\n",
    "# get relevant data stuff\n",
    "df_file = pd.ExcelFile(\"~/spatial/data/messi.xlsx\")\n",
    "messi_df = pd.read_excel(df_file, \"All.Pairs\")\n",
    "merfish_df = pd.read_csv(\"~/spatial/data/raw/merfish.csv\")\n",
    "merfish_df = merfish_df.drop(['Blank_1', 'Blank_2', 'Blank_3', 'Blank_4', 'Blank_5', 'Fos'], axis=1)\n",
    "\n",
    "# these are the 13 ligands or receptors found in MESSI\n",
    "non_response_genes = ['Cbln1', 'Cxcl14', 'Crhbp', 'Gabra1', 'Cbln2', 'Gpr165', \n",
    "                      'Glra3', 'Gabrg1', 'Adora2a', 'Vgf', 'Scg2', 'Cartpt',\n",
    "                      'Tac2']\n",
    "# this list stores the control genes aka \"Blank_{int}\"\n",
    "blank_genes = []\n",
    "\n",
    "# we will populate all of the non-response genes as being in one or the other\n",
    "# the ones already filled in come from the existing 13 L/R genes above\n",
    "ligands = [\"Cbln1\", \"Cxcl14\", \"Cbln2\", \"Vgf\", \"Scg2\", \"Cartpt\", \"Tac2\"]\n",
    "receptors = [\"Crhbp\", \"Gabra1\", \"Gpr165\", \"Glra3\", \"Gabrg1\", \"Adora2a\"]\n",
    "\n",
    "# ligands and receptor indexes in MERFISH\n",
    "non_response_indeces = [list(merfish_df.columns).index(gene)-9 for gene in non_response_genes]\n",
    "ligand_indeces = [list(merfish_df.columns).index(gene)-9 for gene in ligands]\n",
    "receptor_indeces = [list(merfish_df.columns).index(gene)-9 for gene in receptors]\n",
    "all_pairs_columns = [\n",
    "    \"Ligand.ApprovedSymbol\",\n",
    "    \"Receptor.ApprovedSymbol\",\n",
    "]\n",
    "\n",
    "\n",
    "# for column name in the column names above\n",
    "for column in all_pairs_columns:\n",
    "    for gene in merfish_df.columns:\n",
    "        if (\n",
    "            gene.upper() in list(messi_df[column])\n",
    "            and gene.upper() not in non_response_genes\n",
    "        ):\n",
    "            non_response_genes.append(gene)\n",
    "            non_response_indeces.append(list(merfish_df.columns).index(gene)-9)\n",
    "            if column[0] == \"L\":\n",
    "                ligands.append(gene)\n",
    "                ligand_indeces.append(list(merfish_df.columns).index(gene)-9)\n",
    "            else:\n",
    "                receptors.append(gene)\n",
    "                receptor_indeces.append(list(merfish_df.columns).index(gene)-9)\n",
    "        if gene[:5] == \"Blank\" and gene not in blank_genes:\n",
    "            blank_genes.append(gene)\n",
    "            # non_response_indeces.append(list(merfish_df.columns).index(gene)-9)\n",
    "\n",
    "print(non_response_genes)\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(len(non_response_genes))\n",
    "    + \" genes recognized as either ligands or receptors (including new ones).\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(len(blank_genes))\n",
    "    + \" blank genes.\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(155 - len(blank_genes) - len(non_response_genes))\n",
    "    + \" genes that are treated as response variables.\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(len(ligands))\n",
    "    + \" ligands.\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"There are \"\n",
    "    + str(len(receptors))\n",
    "    + \" receptors.\"\n",
    ")\n",
    "\n",
    "response_indeces = list(set(range(155)) - set(non_response_indeces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(batch_obj):\n",
    "    return [batch_obj.edge_index[:, batch_obj.edge_index[0] == i][1] for i in range(batch_obj.x.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3245af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ligand_sum(data, neighbors_tensor, ligand_indeces):\n",
    "    return torch.tensor([np.array(data.index_select(0, neighbors).index_select(1, torch.tensor(ligand_indeces))) for neighbors in neighbors_tensor]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd8d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_celltypes(cell_behavior_tensor, neighbors_tensor):\n",
    "    # print(f\"There are {(num_classes := cell_behavior_tensor.max() + 1)} different cell types.\")\n",
    "    return [F.one_hot(cell_behavior_tensor.index_select(0, neighbors), num_classes=num_classes) for neighbors in neighbors_tensor] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d835ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celltype_simplex(cell_behavior_tensor, neighbors_tensor):\n",
    "    print(f\"There are {(num_classes := 16)} different cell types.\")\n",
    "    return torch.cat([(torch.mean(1.0*F.one_hot(cell_behavior_tensor.index_select(0, neighbors), num_classes=num_classes), dim=0)).unsqueeze(0) for neighbors in neighbors_tensor], dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34db2f9",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43bdd7d",
   "metadata": {},
   "source": [
    "###### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41298a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = [\"Naive\"]\n",
    "sexes = [\"Female\"]\n",
    "\n",
    "with open('animal_id.json') as json_file:\n",
    "    animals = json.load(json_file)\n",
    "\n",
    "# loss_dict = {}\n",
    "time_dict = {}\n",
    "loss_dict = {}\n",
    "# loss_inhibitory_dict = {}\n",
    "gene_loss_dict = {}\n",
    "\n",
    "for behavior in behaviors:\n",
    "    for sex in sexes:\n",
    "        try:\n",
    "            animal_list = animals[behavior][sex]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        behavior = [behavior]\n",
    "        sex = [sex]\n",
    "        # print(behavior, sex, animal_list)\n",
    "        for animal in animal_list:\n",
    "            start = time.time()\n",
    "            trial_run = FilteredMerfishDataset('data', sexes=sex, behaviors=behavior, test_animal=animal)\n",
    "            print(sex, behavior, animal)\n",
    "            datalist = trial_run.construct_graphs(3, True)\n",
    "            print(len(datalist))\n",
    "            start = time.time()\n",
    "\n",
    "            train_dataset = None\n",
    "\n",
    "            for batch in datalist:\n",
    "                # gene expressions of cell i\n",
    "                x = batch.x\n",
    "                # position coordinate of gene i\n",
    "                pos = batch.pos\n",
    "                bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "                # behavior and cell_type\n",
    "                behavior_and_cell_type = batch.y\n",
    "\n",
    "                # get the neighbors of the current batch\n",
    "                neighbors = get_neighbors(batch)\n",
    "\n",
    "                # get the sum of the ligand expressions for each cell in the current batch\n",
    "                total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "                # get the proportion of celltypes as one-hot encoded vectors\n",
    "                celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "                # combine all the data\n",
    "                X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "#                 scaler = StandardScaler().fit(X)\n",
    "#                 X = torch.tensor(scaler.transform(X))\n",
    "\n",
    "                if train_dataset is None:\n",
    "                    train_dataset = X\n",
    "                    train_Y = x[:, response_indeces]\n",
    "                else:\n",
    "                    train_dataset = torch.cat((train_dataset, X), dim=0)\n",
    "                    train_Y = torch.cat((train_Y, x[:, response_indeces]), dim=0)\n",
    "\n",
    "                print(f\"Batch: {datalist.index(batch)+1}/{len(datalist)}\")\n",
    "                \n",
    "            scaler = StandardScaler().fit(train_dataset)\n",
    "            train_dataset = torch.tensor(scaler.transform(train_dataset))\n",
    "\n",
    "            assert train_dataset.shape[0] == train_Y.shape[0]\n",
    "\n",
    "            test_datalist = trial_run.construct_graphs(3, False)\n",
    "\n",
    "            test_dataset = None\n",
    "\n",
    "            for batch in test_datalist:\n",
    "                # gene expressions of cell i\n",
    "                x = batch.x\n",
    "                # position coordinate of gene i\n",
    "                pos = batch.pos\n",
    "                bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "                # behavior and cell_type\n",
    "                behavior_and_cell_type = batch.y\n",
    "\n",
    "                # get the neighbors of the current batch\n",
    "                neighbors = get_neighbors(batch)\n",
    "\n",
    "                # get the sum of the ligand expressions for each cell in the current batch\n",
    "                total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "                # get the proportion of celltypes as one-hot encoded vectors\n",
    "                celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "                # combine all the data\n",
    "                test_X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "\n",
    "                # standardize the data using TRAINING mean and sd.\n",
    "                test_X = torch.tensor(scaler.transform(test_X))\n",
    "\n",
    "                if test_dataset is None:\n",
    "                    test_dataset = test_X\n",
    "                    test_Y = x[:, response_indeces]\n",
    "                    print(test_dataset.shape[0], test_Y.shape[0])\n",
    "                else:\n",
    "                    test_dataset = torch.cat((test_dataset, test_X), dim=0)\n",
    "                    test_Y = torch.cat((test_Y, x[:, response_indeces]), dim=0)\n",
    "\n",
    "                print(f\"Batch: {test_datalist.index(batch)+1}/{len(test_datalist)}\")\n",
    "\n",
    "            test_dataset = torch.tensor(scaler.transform(test_dataset))\n",
    "            \n",
    "            assert test_dataset.shape[0] == test_Y.shape[0]\n",
    "\n",
    "            model_list = []\n",
    "            MAE_list = []\n",
    "\n",
    "            # for each response gene in our response matrix....\n",
    "            for i in range(train_Y.shape[1]):\n",
    "                print(train_dataset.shape)\n",
    "\n",
    "                # create response gene variables\n",
    "                y_i_train = train_Y[:, i]\n",
    "                y_i_test = test_Y[:, i]\n",
    "                \n",
    "                if i == 0 and behavior == [\"Naive\"] and animal == 1:\n",
    "                    print(f\"trainY Mean: {torch.mean(y_i_train)}\")\n",
    "                    print(f\"testY Mean: {torch.mean(y_i_test)}\")\n",
    "\n",
    "                # create XGBoost Regression Model\n",
    "                model = xgb.XGBRegressor(tree_method=\"gpu_hist\", nthread=1, objective=\"reg:squarederror\", eval_metric=\"mae\")\n",
    "\n",
    "                # fit the regression model and add it to model list\n",
    "                # print(train_dataset.shape, y_i_train.shape)\n",
    "                model.fit(np.array(train_dataset), np.array(y_i_train))\n",
    "                model_list.append((f\"Gene {i}\", model_list))\n",
    "\n",
    "                # run the testing data through the model\n",
    "                test_output = torch.tensor(model.predict(np.array(test_dataset)))\n",
    "\n",
    "                # collect its MAE\n",
    "                MAE_list.append(F.l1_loss(test_output, y_i_test))\n",
    "                print(MAE_list[-1])\n",
    "                print(f\"Response Gene {merfish_df.columns[9:][response_indeces[i]]} MAE: {MAE_list[-1].item()}\")\n",
    "                gene_loss_dict[merfish_df.columns[9:][response_indeces[i]]] = MAE_list[-1].item()\n",
    "                print(f\"Response Gene: {i+1}/{train_Y.shape[1]}\")\n",
    "\n",
    "            end = time.time()\n",
    "            time_dict[f\"{sex}_{behavior}_{animal}\"] = end-start\n",
    "            loss_dict[f\"{sex}_{behavior}_{animal}\"] = float(np.mean(MAE_list))\n",
    "\n",
    "            with open(\"XGBoost_time_all.json\", \"w\") as outfile:\n",
    "                json.dump(time_dict, outfile, indent=4)\n",
    "\n",
    "            with open(\"XGBoost_MAE.json\", \"w\") as outfile:\n",
    "                json.dump(loss_dict, outfile, indent=4)\n",
    "\n",
    "            print(f\"Test animal {animal} CV finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a0f26",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_genes=['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2']\n",
    "\n",
    "import time\n",
    "import json\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_MAEs = []\n",
    "\n",
    "time_dict = {}\n",
    "L1_loss_dict = {}\n",
    "\n",
    "for animal in [1,2,3,4]:\n",
    "    start = time.time()\n",
    "    MAE_list = []\n",
    "    for target_gene in response_genes:\n",
    "        neighset=genetypes['ligands']\n",
    "        oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "        # oset=neighset\n",
    "\n",
    "        # oset=[]\n",
    "        # neighset=[]\n",
    "        \n",
    "        train_animals = [1,2,3,4]\n",
    "        train_animals.remove(animal)\n",
    "        print(train_animals)\n",
    "        # FIX THIS SO THAT ONLY FIRST 4 ANIMALS GET USED\n",
    "        trainX,trainY=construct_problem((ad.obs['Animal_ID']!=animal)&(ad.obs['Animal_ID']<=4),target_gene,neighset,oset,filter_excitatory=True)\n",
    "        testX,testY=construct_problem((ad.obs['Animal_ID']==animal),target_gene,neighset,oset,filter_excitatory=True)\n",
    "\n",
    "        print(trainX.shape,trainY.shape)\n",
    "        print(testX.shape,testY.shape)\n",
    "\n",
    "        # whiten covariates\n",
    "        scaler = StandardScaler().fit(trainX)\n",
    "        trainX = scaler.transform(trainX)\n",
    "        testX = scaler.transform(testX)\n",
    "        \n",
    "        model=HistGradientBoostingRegressor(loss=\"absolute_error\")\n",
    "        model.fit(trainX,trainY)\n",
    "        MAE_list.append(np.mean(np.abs(model.predict(testX)-testY)))\n",
    "\n",
    "    end = time.time()\n",
    "    time_dict[f\"Female_Naive_{animal}\"] = end-start\n",
    "    L1_loss_dict[f\"Female_Naive_{animal}\"] = float(np.mean(MAE_list))\n",
    "\n",
    "    with open(\"XGBoost_L1_time_excitatory.json\", \"w\") as outfile:\n",
    "        json.dump(time_dict, outfile, indent=4)\n",
    "\n",
    "    with open(\"XGBoost_L1_MAE_excitatory.json\", \"w\") as outfile:\n",
    "        json.dump(L1_loss_dict, outfile, indent=4)\n",
    "    \n",
    "    all_MAEs.append(np.mean(MAE_list))\n",
    "    \n",
    "print(np.mean(all_MAEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca7682",
   "metadata": {},
   "source": [
    "##### deepST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_list = [1,2,3,4]\n",
    "loss_dict = {}\n",
    "\n",
    "for animal in animal_list:\n",
    "    with initialize(config_path=\"../config\"):\n",
    "        overrides_train = {\n",
    "            \"datasets\": \"FilteredMerfishDataset\",\n",
    "            \"gpus\": \"[4]\"\n",
    "        }\n",
    "        overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "        cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "        # update the behavior to get the model of interest\n",
    "        OmegaConf.update(cfg_from_terminal, \"datasets.dataset.test_animal\", animal)\n",
    "        model, trainer = train(cfg_from_terminal)\n",
    "        output = test(cfg_from_terminal)\n",
    "        trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "        loss_dict.append((animal, test_results[0]['test_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f08e8",
   "metadata": {},
   "source": [
    "##### MESSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "merfish_df = pd.read_csv('../../spatial/data/raw/merfish.csv')\n",
    "sexes = merfish_df[\"Animal_sex\"].unique()\n",
    "behaviors = merfish_df[\"Behavior\"].unique()\n",
    "celltypes = merfish_df[\"Cell_class\"].unique()\n",
    "cell_categories = list(it.product(sexes, behaviors, celltypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-priority",
   "metadata": {},
   "source": [
    "Running MESSI with general celltype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-arthritis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "loss_dict = {}\n",
    "time_dict = {}\n",
    "animals = [1,2,3,4]\n",
    "for sex in ['Female']:\n",
    "    for behavior in ['Naive']:\n",
    "        loss_dict[f\"{sex}_{behavior}_general\"]\n",
    "        for animal in animals:\n",
    "            filtered_df = merfish_df[(merfish_df['Animal_sex'] == sex) & (merfish_df['Behavior'] == behavior)]\n",
    "            if len(filtered_df[\"Animal_ID\"].unique()) != 0:\n",
    "                start = time.time()\n",
    "                MAE = MESSI(sex, behavior, 'general', list(set(animals) - set(animal)))\n",
    "                end = time.time()\n",
    "                # time_dict[f\"{sex}_{behavior}_general\"] = end-start\n",
    "                loss_dict[f\"{sex}_{behavior}_general\"] += MAE/len(animals)\n",
    "            \n",
    "MAE_results = json.dumps(loss_dict, indent=4)\n",
    "time_results = json.dumps(time_dict, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1338e",
   "metadata": {},
   "source": [
    "# Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f1153c",
   "metadata": {},
   "source": [
    "###### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02643f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = [\"Naive\"]\n",
    "sexes = [\"Female\"]\n",
    "\n",
    "with open('animal_id.json') as json_file:\n",
    "    animals = json.load(json_file)\n",
    "\n",
    "# loss_dict = {}\n",
    "time_dict = {}\n",
    "loss_excitatory_dict = {}\n",
    "# loss_inhibitory_dict = {}\n",
    "gene_loss_dict = {}\n",
    "\n",
    "for behavior in behaviors:\n",
    "    for sex in sexes:\n",
    "        try:\n",
    "            animal_list = animals[behavior][sex]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        behavior = [behavior]\n",
    "        sex = [sex]\n",
    "        # print(behavior, sex, animal_list)\n",
    "        for animal in animal_list:\n",
    "            start = time.time()\n",
    "            trial_run = FilteredMerfishDataset('data', sexes=sex, behaviors=behavior, test_animal=animal)\n",
    "            print(sex, behavior, animal)\n",
    "            datalist = trial_run.construct_graphs(3, True)\n",
    "            print(len(datalist))\n",
    "            start = time.time()\n",
    "\n",
    "            train_dataset = None\n",
    "\n",
    "            for batch in datalist:\n",
    "                # gene expressions of cell i\n",
    "                x = batch.x\n",
    "                # position coordinate of gene i\n",
    "                pos = batch.pos\n",
    "                bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "                # behavior and cell_type\n",
    "                behavior_and_cell_type = batch.y\n",
    "\n",
    "                # get the neighbors of the current batch\n",
    "                neighbors = get_neighbors(batch)\n",
    "\n",
    "                # get the sum of the ligand expressions for each cell in the current batch\n",
    "                total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "                # get the proportion of celltypes as one-hot encoded vectors\n",
    "                celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "                # combine all the data\n",
    "                X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "                excitatory_cells = (behavior_and_cell_type[:, 1] == 6).nonzero(as_tuple=True)[0]\n",
    "                X = torch.index_select(X, 0, excitatory_cells)\n",
    "#                 scaler = StandardScaler().fit(X)\n",
    "#                 X = torch.tensor(scaler.transform(X))\n",
    "\n",
    "                if train_dataset is None:\n",
    "                    train_dataset = X\n",
    "                    train_Y = torch.index_select(x[:, response_indeces], 0, excitatory_cells)\n",
    "                else:\n",
    "                    train_dataset = torch.cat((train_dataset, X), dim=0)\n",
    "                    train_Y = torch.cat((train_Y, torch.index_select(x[:, response_indeces], 0, excitatory_cells)), dim=0)\n",
    "\n",
    "                print(f\"Batch: {datalist.index(batch)+1}/{len(datalist)}\")\n",
    "                \n",
    "            scaler = StandardScaler().fit(train_dataset)\n",
    "            train_dataset = torch.tensor(scaler.transform(train_dataset))\n",
    "\n",
    "            assert train_dataset.shape[0] == train_Y.shape[0]\n",
    "\n",
    "            test_datalist = trial_run.construct_graphs(3, False)\n",
    "\n",
    "            test_dataset = None\n",
    "\n",
    "            for batch in test_datalist:\n",
    "                # gene expressions of cell i\n",
    "                x = batch.x\n",
    "                # position coordinate of gene i\n",
    "                pos = batch.pos\n",
    "                bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "                # behavior and cell_type\n",
    "                behavior_and_cell_type = batch.y\n",
    "\n",
    "                # get the neighbors of the current batch\n",
    "                neighbors = get_neighbors(batch)\n",
    "\n",
    "                # get the sum of the ligand expressions for each cell in the current batch\n",
    "                total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "                # get the proportion of celltypes as one-hot encoded vectors\n",
    "                celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "                # combine all the data\n",
    "                test_X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "\n",
    "                # standardize the data using TRAINING mean and sd.\n",
    "                excitatory_cells = (behavior_and_cell_type[:, 1] == 6).nonzero(as_tuple=True)[0]\n",
    "                test_X = torch.index_select(test_X, 0, excitatory_cells)\n",
    "#                 test_X = torch.tensor(scaler.transform(test_X))\n",
    "\n",
    "                if test_dataset is None:\n",
    "                    test_dataset = test_X\n",
    "                    test_Y = torch.index_select(x[:, response_indeces], 0, excitatory_cells)\n",
    "                    print(test_dataset.shape[0], test_Y.shape[0])\n",
    "                else:\n",
    "                    test_dataset = torch.cat((test_dataset, test_X), dim=0)\n",
    "                    test_Y = torch.cat((test_Y, torch.index_select(x[:, response_indeces], 0, excitatory_cells)), dim=0)\n",
    "\n",
    "                print(f\"Batch: {test_datalist.index(batch)+1}/{len(test_datalist)}\")\n",
    "\n",
    "            test_dataset = torch.tensor(scaler.transform(test_dataset))\n",
    "            \n",
    "            assert test_dataset.shape[0] == test_Y.shape[0]\n",
    "\n",
    "            model_list = []\n",
    "            MAE_list = []\n",
    "\n",
    "            # for each response gene in our response matrix....\n",
    "            for i in range(train_Y.shape[1]):\n",
    "\n",
    "                # create response gene variables\n",
    "                y_i_train = train_Y[:, i]\n",
    "                y_i_test = test_Y[:, i]\n",
    "                \n",
    "                if i == 0 and behavior == [\"Naive\"] and animal == 1:\n",
    "                    print(f\"trainY Mean: {torch.mean(y_i_train)}\")\n",
    "                    print(f\"testY Mean: {torch.mean(y_i_test)}\")\n",
    "\n",
    "                # create XGBoost Regression Model\n",
    "                model = xgb.XGBRegressor(tree_method=\"gpu_hist\", nthread=1, objective=\"reg:squarederror\", eval_metric=\"mae\")\n",
    "\n",
    "                # fit the regression model and add it to model list\n",
    "                # print(train_dataset.shape, y_i_train.shape)\n",
    "                model.fit(np.array(train_dataset), np.array(y_i_train))\n",
    "                model_list.append((f\"Gene {i}\", model_list))\n",
    "\n",
    "                # run the testing data through the model\n",
    "                test_output = torch.tensor(model.predict(np.array(test_dataset)))\n",
    "\n",
    "                # collect its MAE\n",
    "                MAE_list.append(F.l1_loss(test_output, y_i_test))\n",
    "                print(MAE_list[-1])\n",
    "                print(f\"Response Gene {merfish_df.columns[9:][response_indeces[i]]} MAE: {MAE_list[-1].item()}\")\n",
    "                gene_loss_dict[merfish_df.columns[9:][response_indeces[i]]] = MAE_list[-1].item()\n",
    "                print(f\"Response Gene: {i+1}/{train_Y.shape[1]}\")\n",
    "\n",
    "            end = time.time()\n",
    "            time_dict[f\"{sex}_{behavior}_{animal}\"] = end-start\n",
    "            loss_excitatory_dict[f\"{sex}_{behavior}_{animal}\"] = float(np.mean(MAE_list))\n",
    "\n",
    "            with open(\"XGBoost_time_excitatory.json\", \"w\") as outfile:\n",
    "                json.dump(time_dict, outfile, indent=4)\n",
    "\n",
    "            with open(\"XGBoost_MAE_excitatory.json\", \"w\") as outfile:\n",
    "                json.dump(loss_excitatory_dict, outfile, indent=4)\n",
    "\n",
    "            print(f\"Test animal {animal} CV finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a08c1",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_genes=['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2']\n",
    "\n",
    "import time\n",
    "import json\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "all_MAEs = []\n",
    "\n",
    "time_dict = {}\n",
    "L1_loss_dict = {}\n",
    "\n",
    "for animal in [1,2,3,4]:\n",
    "    start = time.time()\n",
    "    MAE_list = []\n",
    "    for target_gene in response_genes:\n",
    "        neighset=genetypes['ligands']\n",
    "        oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "        # oset=neighset\n",
    "\n",
    "        # oset=[]\n",
    "        # neighset=[]\n",
    "        \n",
    "        train_animals = [1,2,3,4]\n",
    "        train_animals.remove(animal)\n",
    "        print(train_animals)\n",
    "        # FIX THIS SO THAT ONLY FIRST 4 ANIMALS GET USED\n",
    "        trainX,trainY=construct_problem((ad.obs['Animal_ID']!=animal)&(ad.obs['Animal_ID']<=4),target_gene,neighset,oset,filter_excitatory=True)\n",
    "        testX,testY=construct_problem((ad.obs['Animal_ID']==animal),target_gene,neighset,oset,filter_excitatory=True)\n",
    "\n",
    "        print(trainX.shape,trainY.shape)\n",
    "        print(testX.shape,testY.shape)\n",
    "\n",
    "        # whiten covariates\n",
    "        scaler = StandardScaler().fit(trainX)\n",
    "        trainX = scaler.transform(trainX)\n",
    "        testX = scaler.transform(testX)\n",
    "        \n",
    "        model=HistGradientBoostingRegressor(loss=\"absolute_error\")\n",
    "        model.fit(trainX,trainY)\n",
    "        MAE_list.append(np.mean(np.abs(model.predict(testX)-testY)))\n",
    "\n",
    "    end = time.time()\n",
    "    time_dict[f\"Female_Naive_{animal}\"] = end-start\n",
    "    L1_loss_dict[f\"Female_Naive_{animal}\"] = float(np.mean(MAE_list))\n",
    "\n",
    "    with open(\"XGBoost_L1_time_excitatory.json\", \"w\") as outfile:\n",
    "        json.dump(time_dict, outfile, indent=4)\n",
    "\n",
    "    with open(\"XGBoost_L1_MAE_excitatory.json\", \"w\") as outfile:\n",
    "        json.dump(L1_loss_dict, outfile, indent=4)\n",
    "    \n",
    "    all_MAEs.append(np.mean(MAE_list))\n",
    "    \n",
    "print(np.mean(all_MAEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071756d4",
   "metadata": {},
   "source": [
    "##### deepST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_list = [1,2,3,4]\n",
    "loss_dict = {}\n",
    "\n",
    "for animal in animal_list:\n",
    "    with initialize(config_path=\"../../config\"):\n",
    "        overrides_train = {\n",
    "            \"datasets\": \"FilteredMerfishDataset\",\n",
    "            \"gpus\": \"[4]\",\n",
    "            \"model.kwargs.celltypes\": [\"Excitatory\"],\n",
    "            \"datasets.dataset.test_animal\": animal,\n",
    "        }\n",
    "        overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "        cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "        model, trainer = train(cfg_from_terminal)\n",
    "        output = test(cfg_from_terminal)\n",
    "        trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "        loss_dict.append((animal, test_results[0]['test_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ed9d0",
   "metadata": {},
   "source": [
    "##### MESSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d8bee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "loss_dict = {}\n",
    "time_dict = {}\n",
    "for sex in ['Female']:\n",
    "    for behavior in ['Naive']:\n",
    "        if len(filtered_df[\"Animal_ID\"].unique()) != 0:\n",
    "            start = time.time()\n",
    "            MAE = MESSI(sex, behavior, 'Excitatory')\n",
    "            end = time.time()\n",
    "            time_dict[f\"{sex}_{behavior}_Excitatory\"] = end-start\n",
    "            loss_dict[f\"{sex}_{behavior}_Excitatory\"] = MAE\n",
    "            \n",
    "MAE_results = json.dumps(loss_dict, indent=4)\n",
    "time_results = json.dumps(time_dict, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0ca0a",
   "metadata": {},
   "source": [
    "# Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79ff0a",
   "metadata": {},
   "source": [
    "###### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f050b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}\n",
    "\n",
    "slices = [[1,4], [1,2,4], [1,2,4,7], [1,2,3,4,5,7]]\n",
    "\n",
    "for i, animal_slices in enumerate(slices):\n",
    "\n",
    "    trial_run = FilteredMerfishDataset('data', animals=animal_slices, test_animal=4)\n",
    "    datalist = trial_run.construct_graphs(3, True)\n",
    "\n",
    "    train_dataset = None\n",
    "\n",
    "    for batch in datalist:\n",
    "        # gene expressions of cell i\n",
    "        x = batch.x\n",
    "        # position coordinate of gene i\n",
    "        pos = batch.pos\n",
    "        bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "        # behavior and cell_type\n",
    "        behavior_and_cell_type = batch.y\n",
    "\n",
    "        # get the neighbors of the current batch\n",
    "        neighbors = get_neighbors(batch)\n",
    "\n",
    "        # get the sum of the ligand expressions for each cell in the current batch\n",
    "        total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "        # get the proportion of celltypes as one-hot encoded vectors\n",
    "        celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "        # combine all the data\n",
    "        X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "    #                 scaler = StandardScaler().fit(X)\n",
    "    #                 X = torch.tensor(scaler.transform(X))\n",
    "\n",
    "        if train_dataset is None:\n",
    "            train_dataset = X\n",
    "            train_Y = x[:, response_indeces]\n",
    "        else:\n",
    "            train_dataset = torch.cat((train_dataset, X), dim=0)\n",
    "            train_Y = torch.cat((train_Y, x[:, response_indeces]), dim=0)\n",
    "\n",
    "        print(f\"Batch: {datalist.index(batch)+1}/{len(datalist)}\")\n",
    "\n",
    "    scaler = StandardScaler().fit(train_dataset)\n",
    "    train_dataset = torch.tensor(scaler.transform(train_dataset))\n",
    "\n",
    "    test_datalist = trial_run.construct_graphs(3, False)\n",
    "\n",
    "    test_dataset = None\n",
    "\n",
    "    for batch in test_datalist:\n",
    "        # gene expressions of cell i\n",
    "        x = batch.x\n",
    "        # position coordinate of gene i\n",
    "        pos = batch.pos\n",
    "        bregma = torch.tensor([batch.bregma]*pos.shape[0]).reshape(-1,1)\n",
    "\n",
    "        # behavior and cell_type\n",
    "        behavior_and_cell_type = batch.y\n",
    "\n",
    "        # get the neighbors of the current batch\n",
    "        neighbors = get_neighbors(batch)\n",
    "\n",
    "        # get the sum of the ligand expressions for each cell in the current batch\n",
    "        total_ligands = get_ligand_sum(x, neighbors, ligand_indeces)\n",
    "\n",
    "        # get the proportion of celltypes as one-hot encoded vectors\n",
    "        celltype_proportions = get_celltype_simplex(behavior_and_cell_type[:, 1], neighbors)\n",
    "\n",
    "        # combine all the data\n",
    "        test_X = torch.cat((x[:, non_response_indeces], total_ligands, pos, bregma, celltype_proportions), dim=1)\n",
    "\n",
    "        # standardize the data using TRAINING mean and sd.\n",
    "        test_X = torch.tensor(scaler.transform(test_X))\n",
    "\n",
    "        if test_dataset is None:\n",
    "            test_dataset = test_X\n",
    "            test_Y = x[:, response_indeces]\n",
    "            print(test_dataset.shape[0], test_Y.shape[0])\n",
    "        else:\n",
    "            test_dataset = torch.cat((test_dataset, test_X), dim=0)\n",
    "            test_Y = torch.cat((test_Y, x[:, response_indeces]), dim=0)\n",
    "\n",
    "        print(f\"Batch: {test_datalist.index(batch)+1}/{len(test_datalist)}\")\n",
    "\n",
    "    test_dataset = torch.tensor(scaler.transform(test_dataset))\n",
    "\n",
    "    model_list = []\n",
    "    MAE_list = []\n",
    "\n",
    "    # for each response gene in our response matrix....\n",
    "    for i in range(train_Y.shape[1]):\n",
    "        print(train_dataset.shape)\n",
    "\n",
    "        # create response gene variables\n",
    "        y_i_train = train_Y[:, i]\n",
    "        y_i_test = test_Y[:, i]\n",
    "\n",
    "        # create XGBoost Regression Model\n",
    "        model = xgb.XGBRegressor(tree_method=\"gpu_hist\", nthread=1, objective=\"reg:squarederror\", eval_metric=\"mae\")\n",
    "\n",
    "        # fit the regression model and add it to model list\n",
    "        # print(train_dataset.shape, y_i_train.shape)\n",
    "        model.fit(np.array(train_dataset), np.array(y_i_train))\n",
    "        model_list.append((f\"Gene {i}\", model_list))\n",
    "\n",
    "        # run the testing data through the model\n",
    "        test_output = torch.tensor(model.predict(np.array(test_dataset)))\n",
    "\n",
    "        # collect its MAE\n",
    "        MAE_list.append(F.l1_loss(test_output, y_i_test))\n",
    "        print(MAE_list[-1])\n",
    "        print(f\"Response Gene {merfish_df.columns[9:][response_indeces[i]]} MAE: {MAE_list[-1].item()}\")\n",
    "        gene_loss_dict[merfish_df.columns[9:][response_indeces[i]]] = MAE_list[-1].item()\n",
    "        print(f\"Response Gene: {i+1}/{train_Y.shape[1]}\")\n",
    "\n",
    "    end = time.time()\n",
    "    time_dict[(i+1)*12] = end-start\n",
    "\n",
    "time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f50861d",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6877afa",
   "metadata": {},
   "source": [
    "CONSTRUCT EACH PROBLEM FOR THE TIMING MANUALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighset=genetypes['ligands']\n",
    "oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "# oset=neighset\n",
    "\n",
    "# oset=[]\n",
    "# neighset=[]\n",
    "\n",
    "trainX,trainY,feature_names=construct_problem((ad.obs['Animal_ID']>=2)&(ad.obs['Animal_ID']<=4),'Th',neighset,oset)\n",
    "testX,testY,feature_names=construct_problem((ad.obs['Animal_ID']==1),'Th',neighset,oset)\n",
    "\n",
    "print(trainX.shape,trainY.shape)\n",
    "print(testX.shape,testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e714cb",
   "metadata": {},
   "source": [
    "##### deepST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd1b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}\n",
    "\n",
    "train_animal_sets = [[1,4], [1,2,4], [1,2,4,7], [1,2,3,4,5,7]]\n",
    "\n",
    "for i, animal_slices in enumerate(train_animal_sets):\n",
    "    start = time.time()\n",
    "    with initialize(config_path=\"../config\"):\n",
    "        overrides_train = {\n",
    "            \"datasets\": \"FilteredMerfishDataset\",\n",
    "            \"gpus\": \"[4]\"\n",
    "        }\n",
    "        overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "        cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "        # update the behavior to get the model of interest\n",
    "        OmegaConf.update(cfg_from_terminal, \"datasets.dataset.animals\", animal_slices)\n",
    "        OmegaConf.update(cfg_from_terminal, \"datasets.dataset.test_animal\", 4)\n",
    "        model, trainer = train(cfg_from_terminal)\n",
    "    end = time.time()\n",
    "    time_dict[(i+1)*12] = end-start\n",
    "    \n",
    "time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4eac20",
   "metadata": {},
   "source": [
    "##### MESSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485fe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "[3], [1], [1,3], [1,2], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}\n",
    "# has 6, 12, 18, 24, and 30 slices combined, respectively\n",
    "train_animal_sets = [[1,2,3]]\n",
    "sex = \"Female\"\n",
    "behavior = \"Naive\"\n",
    "for i, animal_slices in enumerate(train_animal_sets):\n",
    "    start = time.time()\n",
    "    MAE = MESSI(sex, behavior, 'general', animal_slices)\n",
    "    end = time.time()\n",
    "    time_dict[(i+1)*6] = end-start\n",
    "\n",
    "print(time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858beada",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391f384e",
   "metadata": {},
   "source": [
    "- [ ] Add the time in for 12 and 24 since 36 and 48 are impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e682755",
   "metadata": {},
   "source": [
    "# Table 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d1b78",
   "metadata": {},
   "source": [
    "##### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101d41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_rad_dict = {}\n",
    "\n",
    "# for each radius value....\n",
    "for radius in range(0, 90, 10):\n",
    "    \n",
    "    ad=anndata.read_h5ad(h5ad_location)\n",
    "    row=np.zeros(0,dtype=int)\n",
    "    col=np.zeros(0,dtype=int)\n",
    "    mode=\"rad\"\n",
    "\n",
    "    for tid in tqdm.notebook.tqdm(np.unique(ad.obs['Tissue_ID'])):\n",
    "        good=ad.obs['Tissue_ID']==tid\n",
    "        pos=np.array(ad.obs[good][['Centroid_X','Centroid_Y']])\n",
    "        if mode == \"neighbors\":\n",
    "            if nneigh == 0:\n",
    "                E = csr_matrix(np.eye(pos.shape[0]))\n",
    "            else:\n",
    "                p=sklearn.neighbors.BallTree(pos)\n",
    "                E=sklearn.neighbors.kneighbors_graph(pos,nneigh,mode='connectivity')\n",
    "            col=np.r_[col,idxs[E.tocoo().col]]\n",
    "            row=np.r_[row,idxs[E.tocoo().row]]\n",
    "        if mode == \"rad\":\n",
    "            p=sp.spatial.cKDTree(pos)\n",
    "            E=p.query_ball_point(pos, r=radius, return_sorted=False)\n",
    "        idxs=np.where(good)[0]\n",
    "\n",
    "\n",
    "    E=sp.sparse.coo_matrix((np.ones(len(col)),(row,col)),shape=(len(ad),len(ad))).tocsr()\n",
    "    if mode == \"neighbors\":\n",
    "        anndata.AnnData(E).write_h5ad(connectivity_matrix_template%nneigh)\n",
    "    if mode == \"rad\":\n",
    "        anndata.AnnData(E).write_h5ad(connectivity_matrix_template%radius)\n",
    "    \n",
    "    # load data\n",
    "    ad=anndata.read_h5ad(h5ad_location)\n",
    "    if mode == \"neighbors\":\n",
    "        connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%nneigh).X\n",
    "    if mode == \"rad\":\n",
    "         connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%radius).X\n",
    "    gene_lookup={x:i for (i,x) in enumerate(ad.var.index)}\n",
    "\n",
    "    with open(genetypes_location,'rb') as f:\n",
    "        genetypes=pickle.load(f)\n",
    "    \n",
    "    for single_gene in ['Pak3', 'Slc17a8', 'Nnat', 'Th']:\n",
    "        \n",
    "        neighset=genetypes['ligands']\n",
    "        oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "\n",
    "        trainX,trainY,feature_names=construct_problem((ad.obs['Animal_ID']<=30), single_gene, neighset,oset)\n",
    "        testX,testY,feature_names=construct_problem((ad.obs['Animal_ID']>30), single_gene, neighset,oset)\n",
    "        \n",
    "        # whiten covariates\n",
    "        scaler = StandardScaler().fit(trainX)\n",
    "        trainX = scaler.transform(trainX)\n",
    "        testX = scaler.transform(testX)\n",
    "        \n",
    "        for num_nodes in [10,50,100,250,500,1000,2500]:\n",
    "            for lr in [0.001, 0.01, 0.1]:\n",
    "                for l2 in [0, 1, 10]:\n",
    "                    model=HistGradientBoostingRegressor(loss=\"absolute_error\", max_leaf_nodes=num_nodes, learning_rate=lr, l2_regularization=l2)\n",
    "                    model.fit(trainX,trainY)\n",
    "                    test_loss_rad_dict[(radius, single_gene, num_nodes, lr, l2)] = np.mean(np.abs(model.predict(testX)-testY))\n",
    "                    print((radius, single_gene, num_nodes, lr, l2), test_loss_rad_dict[(radius, single_gene, num_nodes, lr, l2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c457e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_rad_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4607877",
   "metadata": {},
   "source": [
    "##### deepST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ee15f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_rad_dict = {}\n",
    "\n",
    "# for each radius value....\n",
    "for radius in range(0, 90, 10):\n",
    "    \n",
    "    for single_gene in [[93], [116], [142]]:\n",
    "\n",
    "        # setup framework\n",
    "        with initialize(config_path=\"../../config\"):\n",
    "            overrides_train = {\n",
    "                \"datasets\": \"MerfishDataset\",\n",
    "                \"gpus\": \"[5]\",\n",
    "                \"radius\": radius,\n",
    "                \"model.kwargs.response_genes\": single_gene,\n",
    "                \"training.logger_name\": \"figure4deepST\"\n",
    "            }\n",
    "            overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "            cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "\n",
    "            # complete training\n",
    "            model, trainer = train(cfg_from_terminal)\n",
    "\n",
    "            # load the model with the lowest validation loss\n",
    "            validation_setup = train(cfg_from_terminal, validate_only=True)\n",
    "\n",
    "            # run that model on testing data\n",
    "            output = test(cfg_from_terminal)\n",
    "            trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "            test_loss_rad_dict[(radius, single_gene[0])] = test_results[0]['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9913bb5",
   "metadata": {},
   "source": [
    "##### deepST General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a2859",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_loss_rad_dict = {}\n",
    "\n",
    "# for each radius value....\n",
    "for radius in range(80, 90, 10):\n",
    "\n",
    "        # setup framework\n",
    "        with initialize(config_path=\"../../config\"):\n",
    "            overrides_train = {\n",
    "                \"datasets\": \"MerfishDataset\",\n",
    "                \"gpus\": \"[6]\",\n",
    "                \"radius\": radius,\n",
    "                \"training.logger_name\": \"figure4deepST_general\"\n",
    "            }\n",
    "            overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "            cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "\n",
    "            \n",
    "            # complete training\n",
    "            model, trainer = train(cfg_from_terminal)\n",
    "\n",
    "            # run that model on testing data\n",
    "            output = test(cfg_from_terminal)\n",
    "            trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "            for single_gene in [[93], [116], [142], [151]]:\n",
    "                test_loss_rad_dict[(radius, single_gene[0])] = torch.mean(np.abs((inputs - gene_expressions)[:, single_gene[0]])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd36d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_rad_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad75b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the event you can't train in one go\n",
    "for radius in range(0, 90, 10):\n",
    "    # setup framework\n",
    "    with initialize(config_path=\"../../config\"):\n",
    "        overrides_train = {\n",
    "            \"datasets\": \"MerfishDataset\",\n",
    "            \"gpus\": \"[6]\",\n",
    "            \"radius\": radius,\n",
    "            \"training.logger_name\": \"figure4deepST_general\"\n",
    "        }\n",
    "        overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "        cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "\n",
    "        # complete training\n",
    "        output = test(cfg_from_terminal)\n",
    "        trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "        for single_gene in [[93], [116], [142], [151]]:\n",
    "            test_loss_rad_dict[(radius, single_gene[0])] = torch.mean(np.abs((inputs - gene_expressions)[:, single_gene[0]])).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5017c0f6",
   "metadata": {},
   "source": [
    "# Table 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065ea70",
   "metadata": {},
   "source": [
    "- [ ] Store training loss in dictionary.\n",
    "- [ ] Store validation loss in dictionary.\n",
    "- [ ] Perform the same analysis for LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3470a7",
   "metadata": {},
   "source": [
    "# deepST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1a852",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepST_train_loss_rad_dict = {}\n",
    "deepST_val_loss_rad_dict = {}\n",
    "deepST_test_loss_rad_dict = {}\n",
    "\n",
    "# for each radius value....\n",
    "for radius in range(0, 90, 10):\n",
    "\n",
    "    # setup framework\n",
    "    with initialize(config_path=\"../../config\"):\n",
    "        overrides_train = {\n",
    "            \"datasets\": \"MerfishDataset\",\n",
    "            \"gpus\": \"[5]\",\n",
    "            \"radius\": radius,\n",
    "            \"training.logger_name\": \"figure4deepST\"\n",
    "        }\n",
    "        overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "        cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "        \n",
    "        # complete training\n",
    "        model, trainer = train(cfg_from_terminal)\n",
    "        # uncomment the beneath code to get the training loss at the END of traing\n",
    "        # deepST_train_loss_rad_dict[radius] = trainer.logged_metrics['train_loss: mae']\n",
    "\n",
    "        # load the model with the lowest validation loss\n",
    "        validation_setup, val_trainer = train(cfg_from_terminal, validate_only=True)\n",
    "        deepST_val_loss_rad_dict[radius] = val_trainer.logged_metrics['val_loss']\n",
    "        \n",
    "        # run that model on testing data\n",
    "        output = test(cfg_from_terminal)\n",
    "        trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "        deepST_test_loss_rad_dict[radius] = test_results[0]['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40487afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_rad_dict, val_loss_rad_dict, test_loss_rad_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f50cd",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c49cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_genes=['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2']\n",
    "\n",
    "import time\n",
    "import json\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "lightgbm_train_loss_dict = {}\n",
    "lightgbm_test_loss_dict = {}\n",
    "# for each radius value....\n",
    "for radius in range(0, 90, 10):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    \n",
    "    ad=anndata.read_h5ad(h5ad_location)\n",
    "    row=np.zeros(0,dtype=int)\n",
    "    col=np.zeros(0,dtype=int)\n",
    "    mode=\"rad\"\n",
    "\n",
    "    for tid in tqdm.notebook.tqdm(np.unique(ad.obs['Tissue_ID'])):\n",
    "        good=ad.obs['Tissue_ID']==tid\n",
    "        pos=np.array(ad.obs[good][['Centroid_X','Centroid_Y']])\n",
    "        if mode == \"neighbors\":\n",
    "            if nneigh == 0:\n",
    "                E = csr_matrix(np.eye(pos.shape[0]))\n",
    "            else:\n",
    "                p=sklearn.neighbors.BallTree(pos)\n",
    "                E=sklearn.neighbors.kneighbors_graph(pos,nneigh,mode='connectivity')\n",
    "            col=np.r_[col,idxs[E.tocoo().col]]\n",
    "            row=np.r_[row,idxs[E.tocoo().row]]\n",
    "        if mode == \"rad\":\n",
    "            p=sp.spatial.cKDTree(pos)\n",
    "            E=p.query_ball_point(pos, r=radius, return_sorted=False)\n",
    "        idxs=np.where(good)[0]\n",
    "\n",
    "\n",
    "    E=sp.sparse.coo_matrix((np.ones(len(col)),(row,col)),shape=(len(ad),len(ad))).tocsr()\n",
    "    if mode == \"neighbors\":\n",
    "        anndata.AnnData(E).write_h5ad(connectivity_matrix_template%nneigh)\n",
    "    if mode == \"rad\":\n",
    "        anndata.AnnData(E).write_h5ad(connectivity_matrix_template%radius)\n",
    "    \n",
    "    # load data\n",
    "    ad=anndata.read_h5ad(h5ad_location)\n",
    "    if mode == \"neighbors\":\n",
    "        connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%nneigh).X\n",
    "    if mode == \"rad\":\n",
    "         connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%radius).X\n",
    "    gene_lookup={x:i for (i,x) in enumerate(ad.var.index)}\n",
    "\n",
    "    with open(genetypes_location,'rb') as f:\n",
    "        genetypes=pickle.load(f)\n",
    "    for target_gene in response_genes:\n",
    "        neighset=genetypes['ligands']\n",
    "        oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "        # oset=neighset\n",
    "\n",
    "        # oset=[]\n",
    "        # neighset=[]\n",
    "\n",
    "        trainX,trainY,feature_names=construct_problem((ad.obs['Animal_ID']<=30),target_gene,neighset,oset,True)\n",
    "        testX,testY,feature_names=construct_problem((ad.obs['Animal_ID']>30),target_gene,neighset,oset,True)\n",
    "\n",
    "        # whiten covariates\n",
    "        mu=np.mean(trainX,axis=0)\n",
    "        sig=np.std(trainX,axis=0)\n",
    "        trainX=(trainX-mu)/sig\n",
    "        testX=(testX-mu)/sig\n",
    "\n",
    "        model=HistGradientBoostingRegressor(loss=\"absolute_error\")\n",
    "        model.fit(trainX,trainY)\n",
    "        train_loss_list.append(np.mean(np.abs(model.predict(trainX)-trainY)))\n",
    "        test_loss_list.append(np.mean(np.abs(model.predict(testX)-testY)))\n",
    "        print(f\"Radius {radius}, Gene {target_gene} done.\")\n",
    "\n",
    "    lightgbm_train_loss_dict[radius] = np.mean(train_loss_list)\n",
    "    lightgbm_test_loss_dict[radius] = np.mean(test_loss_list)\n",
    "    \n",
    "print(train_loss_dict, test_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a411cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm_train = {0: 0.36687154522253007, 10: 0.36704256779382094, 20: 0.3669286524358801, 30: 0.36704890036396814, 40: 0.3671144034553719, 50: 0.36696370154104097, 60: 0.36694528763440054, 70: 0.3670999470507582, 80: 0.3669915129489699}\n",
    "lightgbm_test = {0: 0.3930677145164025, 10: 0.3932398374054737, 20: 0.39316476258093197, 30: 0.3931982199864055, 40: 0.3932709117850623, 50: 0.39314322041464844, 60: 0.3932798993305558, 70: 0.39334263072139725, 80: 0.3931518974793018}\n",
    "deepST_train = {0: 0.3521350920200348, 10: 0.35147982835769653, 20: 0.3492688536643982, 30: 0.3481127917766571, 40: 0.34752157330513, 50: 0.34670382738113403, 60: 0.3451015055179596, 70: 0.3450486958026886, 80: 0.3453604280948639}\n",
    "deepST_test = {0: 0.3500196635723114, 10: 0.35144540667533875, 20: 0.3483979403972626, 30: 0.3469792604446411, 40: 0.3454304039478302, 50: 0.3444381654262543, 60: 0.34396353363990784, 70: 0.34458568692207336, 80: 0.34435102343559265}\n",
    "\n",
    "lightgbm_train_df = pd.DataFrame(lightgbm_train.items(), columns=['Radius', 'LightGBM Train Loss'])\n",
    "lightgbm_test_df = pd.DataFrame(lightgbm_test.items(), columns=['Radius', 'LightGBM Test Loss'])\n",
    "deepST_train_df = pd.DataFrame(deepST_test.items(), columns=['Radius', 'deepST Train Loss'])\n",
    "deepST_test_df = pd.DataFrame(deepST_test.items(), columns=['Radius', 'deepST Test Loss'])\n",
    "\n",
    "results_df = lightgbm_train_df.merge(lightgbm_test_df, on=\"Radius\").merge(deepST_train_df, on=\"Radius\").merge(deepST_test_df, on=\"Radius\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068d50b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\"talk\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lineplot(data=results_df, x=\"Radius\", y=\"LightGBM Train Loss\", marker='o')\n",
    "sns.lineplot(data=results_df, x=\"Radius\", y=\"LightGBM Test Loss\", marker='o')\n",
    "sns.lineplot(data=results_df, x=\"Radius\", y=\"deepST Train Loss\", marker='o')\n",
    "sns.lineplot(data=results_df, x=\"Radius\", y=\"deepST Test Loss\", marker='o')\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c289c1",
   "metadata": {},
   "source": [
    "# 0 vs 60 Radius Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459622c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_genes = [0,\n",
    " 2,\n",
    " 3,\n",
    " 4,\n",
    " 5,\n",
    " 6,\n",
    " 7,\n",
    " 10,\n",
    " 19,\n",
    " 20,\n",
    " 21,\n",
    " 22,\n",
    " 23,\n",
    " 24,\n",
    " 25,\n",
    " 26,\n",
    " 27,\n",
    " 28,\n",
    " 32,\n",
    " 34,\n",
    " 35,\n",
    " 37,\n",
    " 38,\n",
    " 39,\n",
    " 40,\n",
    " 41,\n",
    " 42,\n",
    " 43,\n",
    " 44,\n",
    " 52,\n",
    " 53,\n",
    " 54,\n",
    " 55,\n",
    " 58,\n",
    " 63,\n",
    " 64,\n",
    " 66,\n",
    " 67,\n",
    " 69,\n",
    " 71,\n",
    " 73,\n",
    " 74,\n",
    " 75,\n",
    " 76,\n",
    " 77,\n",
    " 78,\n",
    " 79,\n",
    " 80,\n",
    " 85,\n",
    " 86,\n",
    " 87,\n",
    " 88,\n",
    " 93,\n",
    " 94,\n",
    " 96,\n",
    " 97,\n",
    " 99,\n",
    " 102,\n",
    " 103,\n",
    " 104,\n",
    " 106,\n",
    " 110,\n",
    " 112,\n",
    " 113,\n",
    " 114,\n",
    " 116,\n",
    " 118,\n",
    " 119,\n",
    " 120,\n",
    " 121,\n",
    " 122,\n",
    " 123,\n",
    " 124,\n",
    " 125,\n",
    " 126,\n",
    " 129,\n",
    " 130,\n",
    " 131,\n",
    " 133,\n",
    " 134,\n",
    " 141,\n",
    " 142,\n",
    " 147,\n",
    " 151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625fbd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the event you can't train in one go\n",
    "test_loss_dict = {}\n",
    "for response_gene in response_genes:\n",
    "    for radius in [0, 60]:\n",
    "        # setup framework\n",
    "        with initialize(config_path=\"../../config\"):\n",
    "            overrides_train = {\n",
    "                \"datasets\": \"MerfishDataset\",\n",
    "                \"gpus\": \"[6]\",\n",
    "                \"radius\": radius,\n",
    "                \"training.logger_name\": \"zero_vs_sixty\",\n",
    "                \"model.kwargs.response_genes\": [response_gene]\n",
    "            }\n",
    "            overrides_train_list = [f\"{k}={v}\" for k, v in overrides_train.items()]\n",
    "            cfg_from_terminal = compose(config_name=\"config\", overrides=overrides_train_list)\n",
    "\n",
    "            # complete training\n",
    "            output = test(cfg_from_terminal)\n",
    "            trainer, l1_losses, inputs, gene_expressions, celltypes, test_results = output\n",
    "            test_loss_dict[(radius, response_gene)] = test_results[0]['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de2d0c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/raw/merfish.csv\")\n",
    "data = data.drop([\"Blank_1\", \"Blank_2\", \"Blank_3\", \"Blank_4\", \"Blank_5\", \"Fos\"], axis=1)\n",
    "data = data.iloc[:, 9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_dict_with_names = {(k[0], data.columns[k[1]]): v for k,v in test_loss_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efacf68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_dict_with_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95eed86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zeros_vs_sixties = [(test_loss_dict[(0, response_gene)], test_loss_dict[(60, response_gene)]) for response_gene in response_genes if response_gene <= 119]\n",
    "\n",
    "names = [data.columns[response_gene] for response_gene in response_genes]\n",
    "\n",
    "gene_diff_dict = {}\n",
    "\n",
    "for response_gene in response_genes:\n",
    "    if response_gene <= 119:\n",
    "        gene_diff_dict[data.columns[response_gene]] = test_loss_dict[(0, response_gene)] - test_loss_dict[(60, response_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('gene_diffs.json', 'w') as fp:\n",
    "    json.dump(gene_diff_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52556e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros_vs_sixties = np.array(zeros_vs_sixties)\n",
    "plt.scatter(zeros_vs_sixties[:, 0], zeros_vs_sixties[:, 1], marker=\"x\")\n",
    "plt.axline((0, 0), slope=1, color = \"red\")\n",
    "plt.xlabel(\"Zeros\")\n",
    "plt.ylabel(\"Sixties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e257b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [zeros_vs_sixties[i][0] - zeros_vs_sixties[i][1] for i in range(len(zeros_vs_sixties))]\n",
    "_ = plt.hist(differences, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d36d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_diff_percent_dict = {}\n",
    "\n",
    "for response_gene in response_genes:\n",
    "    if response_gene <= 119:\n",
    "        gene_diff_percent_dict[data.columns[response_gene]] = 100*(test_loss_dict[(0, response_gene)] - test_loss_dict[(60, response_gene)])/test_loss_dict[(0, response_gene)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87568fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_diff_percent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438085b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "percent_differences = [100*(zeros_vs_sixties[i][0] - zeros_vs_sixties[i][1])/zeros_vs_sixties[i][0] for i in range(len(zeros_vs_sixties))]\n",
    "_ = plt.hist(percent_differences, bins=25)\n",
    "# plt.title(\"Percent reduction in MAE for genes \\n in the MERFISH hypothalamus dataset\", fontsize=14)\n",
    "plt.xlabel(\"% Reduction in MAE\", fontsize=12)\n",
    "plt.ylabel(\"Number of Genes\", fontsize=12)\n",
    "plt.annotate(\"Ebf3\", (3.05, 1.2), fontsize=10)\n",
    "plt.annotate(\"Ermn\", (4.6, 1.2), fontsize=10)\n",
    "plt.annotate(\"Cpne5\", (3.6, 1.2), fontsize=10)\n",
    "plt.savefig(\"MLCB.png\", dpi=300)\n",
    "plt.gcf().set_dpi(300)\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ae84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(differences), np.quantile(differences, 0.25), np.median(differences), np.quantile(differences, 0.75), max(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ebdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.012604475021362305/0.32853397727012634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48652a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
