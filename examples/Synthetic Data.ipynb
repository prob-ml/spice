{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import types\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import tqdm\n",
    "from sklearn import neighbors\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "class MerfishDataset(torch_geometric.data.InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "        splits=0,\n",
    "    ):\n",
    "        super().__init__(root)\n",
    "\n",
    "        # non-response genes (columns) in MERFISH\n",
    "        with open(non_response_genes_file, \"r\", encoding=\"utf8\") as genes_file:\n",
    "            self.features = [int(x) for x in genes_file.read().split(\",\")]\n",
    "            genes_file.close()\n",
    "\n",
    "        # response genes (columns in MERFISH)\n",
    "        self.response_genes = list(set(range(155)) - set(self.features))\n",
    "\n",
    "        data_list = self.construct_graphs(\n",
    "            n_neighbors, train, log_transform, neighbor_celltypes, radius, splits\n",
    "        )\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            self.gene_names = h5f[\"gene_names\"][:][~self.bad_genes].astype(\"U\")\n",
    "\n",
    "        self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "    # from https://datadryad.org/stash/dataset/doi:10.5061/dryad.8t8s248\n",
    "    url = \"https://datadryad.org/stash/downloads/file_stream/67671\"\n",
    "\n",
    "    behavior_types = [\n",
    "        \"Naive\",\n",
    "        \"Parenting\",\n",
    "        \"Virgin Parenting\",\n",
    "        \"Aggression to pup\",\n",
    "        \"Aggression to adult\",\n",
    "        \"Mating\",\n",
    "    ]\n",
    "    behavior_lookup = {x: i for (i, x) in enumerate(behavior_types)}\n",
    "    cell_types = [\n",
    "        \"Ambiguous\",\n",
    "        \"Astrocyte\",\n",
    "        \"Endothelial 1\",\n",
    "        \"Endothelial 2\",\n",
    "        \"Endothelial 3\",\n",
    "        \"Ependymal\",\n",
    "        \"Excitatory\",\n",
    "        \"Inhibitory\",\n",
    "        \"Microglia\",\n",
    "        \"OD Immature 1\",\n",
    "        \"OD Immature 2\",\n",
    "        \"OD Mature 1\",\n",
    "        \"OD Mature 2\",\n",
    "        \"OD Mature 3\",\n",
    "        \"OD Mature 4\",\n",
    "        \"Pericytes\",\n",
    "    ]\n",
    "    celltype_lookup = {x: i for (i, x) in enumerate(cell_types)}\n",
    "\n",
    "    bad_genes = np.zeros(161, dtype=bool)\n",
    "    bad_genes[[12, 13, 14, 15, 16, 144]] = True\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [\"merfish.csv\", \"merfish.hdf5\"]\n",
    "\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.csv\")\n",
    "\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish.hdf5\")\n",
    "\n",
    "    def download(self):\n",
    "        # download csv if necessary\n",
    "        if not os.path.exists(self.merfish_csv):\n",
    "            with open(self.merfish_csv, \"wb\") as csvf:\n",
    "                csvf.write(requests.get(self.url).content)\n",
    "\n",
    "        # process csv if necessary\n",
    "        dataframe = pd.read_csv(self.merfish_csv)\n",
    "\n",
    "        with h5py.File(self.merfish_hdf5, \"w\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            for colnm, dtype in zip(dataframe.keys()[:9], dataframe.dtypes[:9]):\n",
    "                if dtype.kind == \"O\":\n",
    "                    data = np.require(dataframe[colnm], dtype=\"S36\")\n",
    "                    h5f.create_dataset(colnm, data=data)\n",
    "                else:\n",
    "                    h5f.create_dataset(colnm, data=np.require(dataframe[colnm]))\n",
    "\n",
    "            expression = np.array(dataframe[dataframe.keys()[9:]]).astype(np.float16)\n",
    "            h5f.create_dataset(\"expression\", data=expression)\n",
    "\n",
    "            gene_names = np.array(dataframe.keys()[9:], dtype=\"S80\")\n",
    "            h5f.create_dataset(\"gene_names\", data=gene_names)\n",
    "            \n",
    "    def data_transform(self, x, split_locations, log_transform):\n",
    "        return torch.log1p(x) if log_transform else x\n",
    "    \n",
    "    def calculate_neighborhood(self, split_locations, radius, n_neighbors):\n",
    "        # only include self edges if n_neighbors is 0\n",
    "        if n_neighbors == 0 and radius is None:\n",
    "            edges = np.concatenate(\n",
    "                [\n",
    "                    np.c_[np.array([i]), np.array([i])]\n",
    "                    for i in range(split_locations.shape[0])\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            if radius is None:\n",
    "                nbrs = neighbors.NearestNeighbors(\n",
    "                    n_neighbors=n_neighbors + 1, algorithm=\"ball_tree\"\n",
    "                )\n",
    "                nbrs.fit(split_locations)\n",
    "                _, kneighbors = nbrs.kneighbors(split_locations)\n",
    "                edges = np.concatenate(\n",
    "                    [\n",
    "                        np.c_[kneighbors[:, 0], kneighbors[:, i]]\n",
    "                        for i in range(n_neighbors + 1)\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "\n",
    "                tree = cKDTree(split_locations)\n",
    "                kneighbors = tree.query_ball_point(\n",
    "                    split_locations, r=radius, return_sorted=False\n",
    "                )\n",
    "                edges = np.concatenate(\n",
    "                    [\n",
    "                        np.c_[\n",
    "                            np.repeat(i, len(kneighbors[i])),\n",
    "                            [x for x in kneighbors[i]],\n",
    "                        ]\n",
    "                        for i in range(len(kneighbors))\n",
    "                    ],\n",
    "                    axis=0,\n",
    "                )\n",
    "\n",
    "        return torch.tensor(edges, dtype=torch.long).T\n",
    "\n",
    "    # pylint: disable=too-many-statements\n",
    "    def construct_graph(\n",
    "        self,\n",
    "        data,\n",
    "        anid,\n",
    "        breg,\n",
    "        n_neighbors,\n",
    "        log_transform,\n",
    "        neighbor_celltypes,\n",
    "        radius,\n",
    "        splits=0,\n",
    "    ):\n",
    "        def get_neighbors(edges, x_shape):\n",
    "            return [edges[:, edges[0] == i][1] for i in range(x_shape)]\n",
    "\n",
    "        def get_celltype_simplex(cell_behavior_tensor, neighbors_tensor):\n",
    "            num_classes = cell_behavior_tensor.max() + 1\n",
    "            return torch.cat(\n",
    "                [\n",
    "                    (\n",
    "                        torch.mean(\n",
    "                            1.0\n",
    "                            * F.one_hot(\n",
    "                                cell_behavior_tensor.index_select(0, neighbors),\n",
    "                                num_classes=num_classes,\n",
    "                            ),\n",
    "                            dim=0,\n",
    "                        )\n",
    "                    ).unsqueeze(0)\n",
    "                    for neighbors in neighbors_tensor\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        # get subset of cells in this slice\n",
    "        good = (data.anids == anid) & (data.bregs == breg)\n",
    "        \n",
    "        \n",
    "        # index 0 is behavior, index 1 is celltype, # last 2 are the location. Will update it\n",
    "        # to be last 3 in the case that bregma is included in location.\n",
    "        data_for_this_slice = np.concatenate((\n",
    "            data.behavior[good].reshape(-1,1), \n",
    "            data.celltypes[good].reshape(-1,1), \n",
    "            data.expression[good], \n",
    "            data.locations[good]\n",
    "        ), axis=1)\n",
    "        \n",
    "        \n",
    "        # graph splitting\n",
    "        graph_splits = [data_for_this_slice]\n",
    "        new_splits = []\n",
    "        while splits != 0:\n",
    "            for graph in graph_splits:\n",
    "                locations = graph[:, -2:].astype(np.float32)\n",
    "                x_split = np.median(locations, 0)[0].astype(np.float32)\n",
    "                x_0 = graph[locations[:, 0] < x_split]\n",
    "                x_1 = graph[locations[:, 0] >= x_split]\n",
    "                x_0_locations = x_0[:, -2:].astype(np.float32)\n",
    "                x_1_locations = x_1[:, -2:].astype(np.float32)\n",
    "                y_split_x_0 = np.median(x_0_locations, 0)[1].astype(np.float32)\n",
    "                y_split_x_1 = np.median(x_1_locations, 0)[1].astype(np.float32)\n",
    "                x_00 = x_0[x_0_locations[:, 1] < y_split_x_0]\n",
    "                x_01 = x_0[x_0_locations[:, 1] >= y_split_x_0]\n",
    "                x_10 = x_1[x_1_locations[:, 1] < y_split_x_1]\n",
    "                x_11 = x_1[x_1_locations[:, 1] >= y_split_x_1]\n",
    "                new_splits += [x_00, x_01, x_10, x_11]\n",
    "            graph_splits = new_splits\n",
    "            new_splits = []\n",
    "            splits -= 1\n",
    "\n",
    "        data_splits = []\n",
    "\n",
    "        for split in graph_splits:\n",
    "            \n",
    "            split_locations = split[:, -2:]\n",
    "            \n",
    "            edges = self.calculate_neighborhood(split_locations, radius, n_neighbors)\n",
    "\n",
    "            # remove gene 144.  which is bad.  for some reason.\n",
    "            subexpression = split[:, 2:-2]\n",
    "            subexpression = subexpression[:, ~self.bad_genes]\n",
    "\n",
    "            # get behavior ids\n",
    "            behavior_ids = np.array(\n",
    "                [self.behavior_lookup[x] for x in split[:, 0]]\n",
    "            )\n",
    "            celltype_ids = np.array(\n",
    "                [self.celltype_lookup[x] for x in split[:, 1]]\n",
    "            )\n",
    "            labelinfo = np.c_[behavior_ids, celltype_ids]\n",
    "\n",
    "            # make it into a torch geometric data object, add it to the list!\n",
    "\n",
    "            # if we want to first log transform the data, we do it here\n",
    "            # make this one return statement only changing x\n",
    "            predictors_x = torch.tensor(subexpression.astype(np.float32))\n",
    "            if neighbor_celltypes:\n",
    "                test_simplex = get_celltype_simplex(\n",
    "                    torch.tensor(labelinfo[:, 1]),\n",
    "                    get_neighbors(edges, predictors_x.shape[0]),\n",
    "                )\n",
    "                predictors_x = torch.cat((predictors_x, test_simplex), dim=1)\n",
    "            predictors_x = self.data_transform(predictors_x, split_locations, log_transform)\n",
    "\n",
    "            data_splits.append(\n",
    "                torch_geometric.data.Data(\n",
    "                    x=predictors_x,\n",
    "                    edge_index=edges,\n",
    "                    pos=torch.tensor(split[:, -2:].astype(\"float\")),\n",
    "                    y=torch.tensor(labelinfo),\n",
    "                    bregma=breg,\n",
    "                    anid=anid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        splits=0,\n",
    "    ):\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "            # see if you can update data locations AFTER data was created\n",
    "            # create a deepcopy and then split the locations\n",
    "\n",
    "            # get the (animal_id,bregma) pairs that define a unique slice\n",
    "            unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "            # are we looking at train or test sets?\n",
    "            # the number for filtering represents animal id\n",
    "            unique_slices = (\n",
    "                unique_slices[unique_slices[:, 0] <= 30]\n",
    "                if train\n",
    "                else unique_slices[unique_slices[:, 0] > 30]\n",
    "            )\n",
    "\n",
    "            # store all the slices in this list...\n",
    "            data_list = []\n",
    "\n",
    "            for anid, breg in tqdm.tqdm(unique_slices):\n",
    "                data_list.append(\n",
    "                    self.construct_graph(\n",
    "                        data,\n",
    "                        anid,\n",
    "                        breg,\n",
    "                        n_neighbors,\n",
    "                        log_transform,\n",
    "                        neighbor_celltypes,\n",
    "                        radius,\n",
    "                        splits=splits,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            return sum(data_list, [])\n",
    "\n",
    "\n",
    "class FilteredMerfishDataset(MerfishDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        n_neighbors=3,\n",
    "        train=True,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        non_response_genes_file=\"/home/roko/spatial/spatial/\"\n",
    "        \"non_response_blank_removed.txt\",\n",
    "        animals=None,\n",
    "        bregmas=None,\n",
    "        sexes=None,\n",
    "        behaviors=None,\n",
    "        test_animal=None,\n",
    "        full_animal_holdout=False,\n",
    "        splits=0,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.animals = animals\n",
    "        self.bregmas = bregmas\n",
    "        self.sexes = sexes\n",
    "        self.behaviors = behaviors\n",
    "        self.test_animal = test_animal\n",
    "        self.full_animal_holdout = full_animal_holdout\n",
    "        original_csv_file = super().merfish_csv\n",
    "        new_df = pd.read_csv(original_csv_file)\n",
    "        # print(f\"Original Data {new_df.shape}\")\n",
    "        if self.sexes is not None:\n",
    "            new_df = new_df[new_df[\"Animal_sex\"].isin(self.sexes)]\n",
    "        if self.behaviors is not None:\n",
    "            new_df = new_df[new_df[\"Behavior\"].isin(self.behaviors)]\n",
    "        if self.animals is not None:\n",
    "            new_df = new_df[new_df[\"Animal_ID\"].isin(self.animals)]\n",
    "        if self.bregmas is not None:\n",
    "            new_df = new_df[new_df[\"Bregma\"].isin(self.bregmas)]\n",
    "        if new_df.shape[0] == 0:\n",
    "            raise ValueError(\"Dataframe has no rows. Cannot build graph.\")\n",
    "        new_df.to_csv(str(self.root) + \"/raw/merfish_messi.csv\", index=False)\n",
    "        print(f\"Filtered Data {new_df.shape}\")\n",
    "        # print(\"Filtered csv file created!\")\n",
    "        MerfishDataset.download(self)\n",
    "        super().__init__(\n",
    "            root,\n",
    "            n_neighbors=n_neighbors,\n",
    "            train=train,\n",
    "            log_transform=log_transform,\n",
    "            neighbor_celltypes=neighbor_celltypes,\n",
    "            non_response_genes_file=non_response_genes_file,\n",
    "            radius=radius,\n",
    "            splits=splits,\n",
    "        )\n",
    "        # print(\"Filtered hdf5 file created!\")\n",
    "\n",
    "    #     @property\n",
    "    #     def raw_file_names(self):\n",
    "    #         return [\"merfish_messi.csv\", \"merfish_messi.hdf5\"]\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_csv(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.csv\")\n",
    "\n",
    "    # THIS LINE WAS EDITED TO SHOW NEW FILE\n",
    "    @property\n",
    "    def merfish_hdf5(self):\n",
    "        return os.path.join(self.raw_dir, \"merfish_messi.hdf5\")\n",
    "\n",
    "    def construct_graphs(\n",
    "        self,\n",
    "        n_neighbors,\n",
    "        train,\n",
    "        log_transform=True,\n",
    "        neighbor_celltypes=False,\n",
    "        radius=None,\n",
    "        splits=0,\n",
    "    ):\n",
    "\n",
    "        # load hdf5\n",
    "        with h5py.File(self.merfish_hdf5, \"r\") as h5f:\n",
    "            # pylint: disable=no-member\n",
    "            data = types.SimpleNamespace(\n",
    "                anids=h5f[\"Animal_ID\"][:],\n",
    "                bregs=h5f[\"Bregma\"][:],\n",
    "                expression=h5f[\"expression\"][:],\n",
    "                locations=np.c_[h5f[\"Centroid_X\"][:], h5f[\"Centroid_Y\"][:]],\n",
    "                behavior=h5f[\"Behavior\"][:].astype(\"U\"),\n",
    "                celltypes=h5f[\"Cell_class\"][:].astype(\"U\"),\n",
    "            )\n",
    "\n",
    "        anid_to_bregma_count = {\n",
    "            1: 12,\n",
    "            2: 12,\n",
    "            3: 6,\n",
    "            4: 5,\n",
    "            5: 6,\n",
    "            6: 6,\n",
    "            7: 12,\n",
    "            8: 6,\n",
    "            9: 6,\n",
    "            10: 6,\n",
    "            11: 6,\n",
    "            12: 4,\n",
    "            13: 4,\n",
    "            14: 4,\n",
    "            15: 4,\n",
    "            16: 4,\n",
    "            17: 4,\n",
    "            18: 4,\n",
    "            19: 4,\n",
    "            20: 4,\n",
    "            21: 4,\n",
    "            22: 4,\n",
    "            23: 4,\n",
    "            24: 4,\n",
    "            25: 4,\n",
    "            26: 4,\n",
    "            27: 2,\n",
    "            28: 4,\n",
    "            29: 4,\n",
    "            30: 4,\n",
    "        }\n",
    "\n",
    "        # get the (animal_id,bregma) pairs that define a unique slice\n",
    "        unique_slices = np.unique(np.c_[data.anids, data.bregs], axis=0)\n",
    "\n",
    "        # are we looking at train or test sets?\n",
    "\n",
    "        # if we want a specific animals\n",
    "        if self.test_animal is not None:\n",
    "\n",
    "            # we need to find which of the slices\n",
    "            sorted_anids = np.sort(np.unique(data.anids))\n",
    "            slices_before_test_anid = 0\n",
    "            for anid in sorted_anids:\n",
    "                if anid != self.test_animal:\n",
    "                    slices_before_test_anid += anid_to_bregma_count[anid]\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            mask_train = np.ones(unique_slices.shape[0], dtype=bool)\n",
    "            mask_train[\n",
    "                slices_before_test_anid : (\n",
    "                    slices_before_test_anid + anid_to_bregma_count[self.test_animal]\n",
    "                )\n",
    "            ] = 0\n",
    "            unique_slices = (\n",
    "                unique_slices[(1 - mask_train).astype(\"bool\")]\n",
    "                if not train\n",
    "                else unique_slices[mask_train]\n",
    "            )\n",
    "\n",
    "        elif self.full_animal_holdout and (\n",
    "            len(self.animals) > 1 or np.unique(unique_slices[:, 0]) > 1\n",
    "        ):\n",
    "            print(f\"BEFORE: {unique_slices}\")\n",
    "            min_animal = anid_to_bregma_count[np.min(data.anids)]\n",
    "            unique_slices = (\n",
    "                unique_slices[min_animal:] if train else unique_slices[:min_animal]\n",
    "            )\n",
    "            print(f\"AFTER: {unique_slices}\")\n",
    "\n",
    "        else:\n",
    "            num_slices = len(unique_slices)\n",
    "            min_holdout = max(1, round((1 / 7) * num_slices))\n",
    "            unique_slices = (\n",
    "                unique_slices[: (num_slices - min_holdout)]\n",
    "                if train\n",
    "                else unique_slices[(num_slices - min_holdout) :]\n",
    "            )\n",
    "\n",
    "        # store all the slices in this list...\n",
    "        data_list = []\n",
    "        for anid, breg in unique_slices:\n",
    "            data_list.append(\n",
    "                self.construct_graph(\n",
    "                    data,\n",
    "                    anid,\n",
    "                    breg,\n",
    "                    n_neighbors,\n",
    "                    log_transform,\n",
    "                    neighbor_celltypes,\n",
    "                    radius,\n",
    "                    splits,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(data_list, [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97125ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset0(MerfishDataset):\n",
    "    \n",
    "    def __gate(self, edges, x, i):\n",
    "        neighboring_gene1_expr = torch.sum(x[edges[:, edges[0] == i][1], 1])\n",
    "        return neighboring_gene1_expr.item() * (neighboring_gene1_expr > 1).item()\n",
    "    \n",
    "    def data_transform(self, x, split_locations, log_transform, true_radius=30, cell_volume=5):\n",
    "        edges = self.calculate_neighborhood(split_locations, radius=true_radius, n_neighbors=None)\n",
    "        x = torch.distributions.negative_binomial.NegativeBinomial(1, 0.5).sample(x.shape)/cell_volume\n",
    "        x[:, 0] = torch.arange(len(x), dtype=torch.float64).apply_(lambda i, edges=edges, x=x: self.__gate(edges, x, i))\n",
    "        return torch.log1p(x) if log_transform else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset1(MerfishDataset):\n",
    "    \n",
    "    def __gate(self, edges, x, i):\n",
    "        neighboring_gene1_expr = torch.sum(x[edges[:, edges[0] == i][1], 1])\n",
    "        return neighboring_gene1_expr.item() * (neighboring_gene1_expr > 1).item()\n",
    "    \n",
    "    def data_transform(self, x, split_locations, log_transform, true_radius=30):\n",
    "        edges = self.calculate_neighborhood(split_locations, radius=true_radius, n_neighbors=None)\n",
    "        x = torch.distributions.exponential.Exponential(10).rsample(x.shape)\n",
    "        x[:, 0] = torch.arange(len(x), dtype=torch.float64).apply_(lambda i, edges=edges, x=x: self.__gate(edges, x, i))\n",
    "        return torch.log1p(x) if log_transform else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72944548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset2(MerfishDataset):\n",
    "    \n",
    "    def __transform(self, edges, x, i):\n",
    "        neighboring_gene_expr = (torch.mean(x[edges[:, edges[0] == i][1], 1:10])).item()\n",
    "        return neighboring_gene_expr * (2 ** np.sign(neighboring_gene_expr - 1.6))\n",
    "    \n",
    "    def data_transform(self, x, split_locations, log_transform, true_radius=30):\n",
    "        edges = self.calculate_neighborhood(split_locations, radius=true_radius, n_neighbors=None)\n",
    "        x = torch.exp(torch.distributions.normal.Normal(0, 1).rsample(x.shape))\n",
    "        x[:, 0] += torch.arange(len(x), dtype=torch.float64).apply_(lambda i, edges=edges, x=x: self.__transform(edges, x, i))\n",
    "        return torch.log1p(x) if log_transform else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4debeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset3(MerfishDataset):\n",
    "    \n",
    "    def __transform(self, edges, x, i):\n",
    "        average_gene1_expr = torch.mean(x[:, 1])\n",
    "        average_gene2_expr = torch.mean(x[:, 2])\n",
    "        neighboring_gene1_expr = torch.mean(x[edges[:, edges[0] == i][1], 1])\n",
    "        neighboring_gene2_expr = torch.mean(x[edges[:, edges[0] == i][1], 2])\n",
    "        return 2 ** (1 if torch.sign(neighboring_gene1_expr - average_gene1_expr) - torch.sign(neighboring_gene2_expr - average_gene2_expr) > 0 else -1)\n",
    "    \n",
    "    def data_transform(self, x, split_locations, log_transform, true_radius=30):\n",
    "        edges = self.calculate_neighborhood(split_locations, radius=true_radius, n_neighbors=None)\n",
    "        x = torch.exp(torch.distributions.normal.Normal(0, 0.25).rsample(x.shape))\n",
    "        x[:, 0] *= torch.arange(len(x), dtype=torch.float64).apply_(lambda i, edges=edges, x=x: self.__transform(edges, x, i))\n",
    "        return torch.log1p(x) if log_transform else x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa879b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MerfishDataset3D(MerfishDataset):\n",
    "    \n",
    "    # pylint: disable=too-many-statements\n",
    "    def construct_graph(\n",
    "        self,\n",
    "        data,\n",
    "        anid,\n",
    "        breg,\n",
    "        n_neighbors,\n",
    "        log_transform,\n",
    "        neighbor_celltypes,\n",
    "        radius,\n",
    "        splits=0,\n",
    "    ):\n",
    "        def get_neighbors(edges, x_shape):\n",
    "            return [edges[:, edges[0] == i][1] for i in range(x_shape)]\n",
    "\n",
    "        def get_celltype_simplex(cell_behavior_tensor, neighbors_tensor):\n",
    "            num_classes = cell_behavior_tensor.max() + 1\n",
    "            return torch.cat(\n",
    "                [\n",
    "                    (\n",
    "                        torch.mean(\n",
    "                            1.0\n",
    "                            * F.one_hot(\n",
    "                                cell_behavior_tensor.index_select(0, neighbors),\n",
    "                                num_classes=num_classes,\n",
    "                            ),\n",
    "                            dim=0,\n",
    "                        )\n",
    "                    ).unsqueeze(0)\n",
    "                    for neighbors in neighbors_tensor\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        # get subset of cells in this slice\n",
    "        good = (data.anids == anid)\n",
    "        \n",
    "        \n",
    "        # index 0 is behavior, index 1 is celltype, # last 2 are the location. Will update it\n",
    "        # to be last 3 in the case that bregma is included in location.\n",
    "        data_for_this_slice = np.concatenate((\n",
    "            data.behavior[good].reshape(-1,1), \n",
    "            data.celltypes[good].reshape(-1,1), \n",
    "            data.expression[good], \n",
    "            data.locations[good],\n",
    "            data.bregs[good].reshape(-1,1)\n",
    "        ), axis=1)\n",
    "        \n",
    "        \n",
    "        # graph splitting\n",
    "        graph_splits = [data_for_this_slice]\n",
    "        new_splits = []\n",
    "        while splits != 0:\n",
    "            for graph in graph_splits:\n",
    "                locations = graph[:, -3:].astype(np.float32)\n",
    "                x_split = np.median(locations, 0)[0].astype(np.float32)\n",
    "                x_0 = graph[locations[:, 0] < x_split]\n",
    "                x_1 = graph[locations[:, 0] >= x_split]\n",
    "                x_0_locations = x_0[:, -3:].astype(np.float32)\n",
    "                x_1_locations = x_1[:, -3:].astype(np.float32)\n",
    "                y_split_x_0 = np.median(x_0_locations, 0)[1].astype(np.float32)\n",
    "                y_split_x_1 = np.median(x_1_locations, 0)[1].astype(np.float32)\n",
    "                x_00 = x_0[x_0_locations[:, 1] < y_split_x_0]\n",
    "                x_01 = x_0[x_0_locations[:, 1] >= y_split_x_0]\n",
    "                x_10 = x_1[x_1_locations[:, 1] < y_split_x_1]\n",
    "                x_11 = x_1[x_1_locations[:, 1] >= y_split_x_1]\n",
    "                x_00_locations = x_00[:, -3:].astype(np.float32)\n",
    "                x_01_locations = x_01[:, -3:].astype(np.float32)\n",
    "                x_10_locations = x_10[:, -3:].astype(np.float32)\n",
    "                x_11_locations = x_11[:, -3:].astype(np.float32)\n",
    "                z_split_x_0_y_0 = np.median(x_00_locations, 0)[2].astype(np.float32)\n",
    "                z_split_x_0_y_1 = np.median(x_01_locations, 0)[2].astype(np.float32)\n",
    "                z_split_x_1_y_0 = np.median(x_10_locations, 0)[2].astype(np.float32)\n",
    "                z_split_x_1_y_1 = np.median(x_11_locations, 0)[2].astype(np.float32)\n",
    "                x_000 = x_00[x_00_locations[:, 2] < z_split_x_0_y_0]\n",
    "                x_001 = x_00[x_00_locations[:, 2] >= z_split_x_0_y_0]\n",
    "                x_010 = x_01[x_01_locations[:, 2] < z_split_x_0_y_1]\n",
    "                x_011 = x_01[x_01_locations[:, 2] >= z_split_x_0_y_1]\n",
    "                x_100 = x_10[x_10_locations[:, 2] < z_split_x_1_y_0]\n",
    "                x_101 = x_10[x_10_locations[:, 2] >= z_split_x_1_y_0]\n",
    "                x_110 = x_11[x_11_locations[:, 2] < z_split_x_1_y_1]\n",
    "                x_111 = x_11[x_11_locations[:, 2] >= z_split_x_1_y_1]\n",
    "                new_splits += [x_000, x_001, x_010, x_011, x_100, x_101, x_110, x_111]\n",
    "            graph_splits = new_splits\n",
    "            new_splits = []\n",
    "            splits -= 1\n",
    "\n",
    "        data_splits = []\n",
    "\n",
    "        for split in graph_splits:\n",
    "            \n",
    "            split_locations = split[:, -3:]\n",
    "            \n",
    "            edges = self.calculate_neighborhood(split_locations, radius, n_neighbors)\n",
    "\n",
    "            # remove gene 144.  which is bad.  for some reason.\n",
    "            subexpression = split[:, 2:-3]\n",
    "            subexpression = subexpression[:, ~self.bad_genes]\n",
    "\n",
    "            # get behavior ids\n",
    "            behavior_ids = np.array(\n",
    "                [self.behavior_lookup[x] for x in split[:, 0]]\n",
    "            )\n",
    "            celltype_ids = np.array(\n",
    "                [self.celltype_lookup[x] for x in split[:, 1]]\n",
    "            )\n",
    "            labelinfo = np.c_[behavior_ids, celltype_ids]\n",
    "\n",
    "            # make it into a torch geometric data object, add it to the list!\n",
    "\n",
    "            # if we want to first log transform the data, we do it here\n",
    "            # make this one return statement only changing x\n",
    "            predictors_x = torch.tensor(subexpression.astype(np.float32))\n",
    "            if neighbor_celltypes:\n",
    "                test_simplex = get_celltype_simplex(\n",
    "                    torch.tensor(labelinfo[:, 1]),\n",
    "                    get_neighbors(edges, predictors_x.shape[0]),\n",
    "                )\n",
    "                predictors_x = torch.cat((predictors_x, test_simplex), dim=1)\n",
    "            predictors_x = self.data_transform(predictors_x, split_locations, log_transform)\n",
    "\n",
    "            data_splits.append(\n",
    "                torch_geometric.data.Data(\n",
    "                    x=predictors_x,\n",
    "                    edge_index=edges,\n",
    "                    pos=torch.tensor(split[:, -3:].astype(\"float\")),\n",
    "                    y=torch.tensor(labelinfo),\n",
    "                    anid=anid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test = MerfishDataset('../data', splits = 0, radius = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d837a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = SyntheticDataset0('../data', splits = 0, radius = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312cde5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(test0[0].x[:, 0] > 0)/len(test0[0].x[:, 0]) # should be 0.5 +- 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = SyntheticDataset1('../data', splits = 0, radius = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962534c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = SyntheticDataset2('../data', splits = 2, radius = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e824d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = SyntheticDataset3('../data', splits = 0, radius = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010dd9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3d = MerfishDataset3D('../data', splits = 0, radius = 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
