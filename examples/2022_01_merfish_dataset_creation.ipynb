{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anndata\n",
      "  Downloading anndata-0.7.8-py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from anndata) (3.6.0)\n",
      "Collecting natsort\n",
      "  Downloading natsort-8.0.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: packaging>=20 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from anndata) (21.3)\n",
      "Requirement already satisfied: pandas>=1.1.1 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from anndata) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from anndata) (1.22.0)\n",
      "Collecting xlrd<2.0\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 48.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>1.4 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from anndata) (1.7.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from packaging>=20->anndata) (3.0.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from pandas>=1.1.1->anndata) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from pandas>=1.1.1->anndata) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.1->anndata) (1.16.0)\n",
      "Installing collected packages: xlrd, natsort, anndata\n",
      "Successfully installed anndata-0.7.8 natsort-8.0.2 xlrd-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/roko/.cache/pypoetry/virtualenvs/spatial-G_n0JvVf-py3.8/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import h5py\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import scipy.sparse.linalg\n",
    "rng=np.random.default_rng()\n",
    "import tqdm.notebook\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import ipywidgets\n",
    "import sklearn.neighbors\n",
    "\n",
    "original_url= \"https://datadryad.org/stash/downloads/file_stream/67671\"\n",
    "csv_location='/data/spatial/moffit_merfish/original_file.csv'\n",
    "h5ad_location='/data/spatial/moffit_merfish/original_file.h5ad'\n",
    "connectivity_matrix_template='/data/spatial/moffit_merfish/connectivity_%dneighbors.h5ad'\n",
    "genetypes_location='/data/spatial/moffit_merfish/genetypes.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open(csv_location, \"wb\") as csvf:\n",
    "    csvf.write(requests.get(self.url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# munge into hdf5 file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataframe = pd.read_csv(csv_location)\n",
    "\n",
    "dct={}\n",
    "for colnm, dtype in zip(dataframe.keys()[:9], dataframe.dtypes[:9]):\n",
    "    if dtype.kind == \"O\":\n",
    "        dct[colnm]=np.require(dataframe[colnm], dtype=\"U36\")\n",
    "    else:\n",
    "        dct[colnm]=np.require(dataframe[colnm])\n",
    "expression = np.array(dataframe[dataframe.keys()[9:]]).astype(np.float16)\n",
    "gene_names = np.array(dataframe.keys()[9:], dtype=\"U80\")\n",
    "cellid=dct.pop('Cell_ID')\n",
    "\n",
    "ad=anndata.AnnData(\n",
    "    X=expression,\n",
    "    var=pd.DataFrame(index=gene_names),\n",
    "    obs=pd.DataFrame(dct,index=cellid)\n",
    ")\n",
    "\n",
    "ad.write_h5ad(h5ad_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# supplement hdf5 file with a column indicating \"tissue id\" for each cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "animal_ids=np.unique(ad.obs['Animal_ID'])\n",
    "bregmas=np.unique(ad.obs['Bregma'])\n",
    "tissue_id=np.zeros(len(ad),dtype=int)\n",
    "n_tissues=0\n",
    "    \n",
    "for aid in animal_ids:\n",
    "    for bregma in bregmas:\n",
    "        good=(ad.obs['Animal_ID']==aid)&(ad.obs['Bregma']==bregma)\n",
    "        if np.sum(good)>0:\n",
    "            tissue_id[good]=n_tissues\n",
    "            n_tissues+=1\n",
    "ad.obs['Tissue_ID']=tissue_id\n",
    "ad.write_h5ad(h5ad_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create global graph (using 3 nearest neigbors)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "row=np.zeros(0,dtype=int)\n",
    "col=np.zeros(0,dtype=int)\n",
    "nneigh=3\n",
    "\n",
    "for tid in tqdm.notebook.tqdm(np.unique(ad.obs['Tissue_ID'])):\n",
    "    good=ad.obs['Tissue_ID']==tid\n",
    "    pos=np.array(ad.obs[good][['Centroid_X','Centroid_Y']])\n",
    "    p=sklearn.neighbors.BallTree(pos)\n",
    "    E=sklearn.neighbors.kneighbors_graph(pos,nneigh,mode='connectivity')\n",
    "    idxs=np.where(good)[0]\n",
    "    col=np.r_[col,idxs[E.tocoo().col]]\n",
    "    row=np.r_[row,idxs[E.tocoo().row]]\n",
    "    \n",
    "E=sp.sparse.coo_matrix((np.ones(len(col)),(row,col)),shape=(len(ad),len(ad))).tocsr()\n",
    "anndata.AnnData(E).write_h5ad(connectivity_matrix_template%nneigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write down ligand/receptor sets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ligands=np.array(['Cbln1', 'Cxcl14', 'Cbln2', 'Vgf', 'Scg2', 'Cartpt', 'Tac2',\n",
    "       'Bdnf', 'Bmp7', 'Cyr61', 'Fn1', 'Fst', 'Gad1', 'Ntng1', 'Pnoc',\n",
    "       'Selplg', 'Sema3c', 'Sema4d', 'Serpine1', 'Adcyap1', 'Cck', 'Crh',\n",
    "       'Gal', 'Gnrh1', 'Nts', 'Oxt', 'Penk', 'Sst', 'Tac1', 'Trh', 'Ucn3'])\n",
    "\n",
    "receptors=np.array(['Crhbp', 'Gabra1', 'Gpr165', 'Glra3', 'Gabrg1', 'Adora2a',\n",
    "       'Avpr1a', 'Avpr2', 'Brs3', 'Calcr', 'Cckar', 'Cckbr', 'Crhr1',\n",
    "       'Crhr2', 'Galr1', 'Galr2', 'Grpr', 'Htr2c', 'Igf1r', 'Igf2r',\n",
    "       'Kiss1r', 'Lepr', 'Lpar1', 'Mc4r', 'Npy1r', 'Npy2r', 'Ntsr1',\n",
    "       'Oprd1', 'Oprk1', 'Oprl1', 'Oxtr', 'Pdgfra', 'Prlr', 'Ramp3',\n",
    "       'Rxfp1', 'Slc17a7', 'Slc18a2', 'Tacr1', 'Tacr3', 'Trhr'])\n",
    "\n",
    "response_genes=np.array(['Ace2', 'Aldh1l1', 'Amigo2', 'Ano3', 'Aqp4', 'Ar', 'Arhgap36',\n",
    "       'Baiap2', 'Ccnd2', 'Cd24a', 'Cdkn1a', 'Cenpe', 'Chat', 'Coch',\n",
    "       'Col25a1', 'Cplx3', 'Cpne5', 'Creb3l1', 'Cspg5', 'Cyp19a1',\n",
    "       'Cyp26a1', 'Dgkk', 'Ebf3', 'Egr2', 'Ermn', 'Esr1', 'Etv1',\n",
    "       'Fbxw13', 'Fezf1', 'Fos', 'Gbx2', 'Gda', 'Gem', 'Gjc3', 'Greb1',\n",
    "       'Irs4', 'Isl1', 'Klf4', 'Krt90', 'Lmod1', 'Man1a', 'Mbp', 'Mki67',\n",
    "       'Mlc1', 'Myh11', 'Ndnf', 'Ndrg1', 'Necab1', 'Nnat', 'Nos1',\n",
    "       'Npas1', 'Nup62cl', 'Omp', 'Onecut2', 'Opalin', 'Pak3', 'Pcdh11x',\n",
    "       'Pgr', 'Plin3', 'Pou3f2', 'Rgs2', 'Rgs5', 'Rnd3', 'Scgn',\n",
    "       'Serpinb1b', 'Sgk1', 'Slc15a3', 'Slc17a6', 'Slc17a8', 'Slco1a4',\n",
    "       'Sln', 'Sox4', 'Sox6', 'Sox8', 'Sp9', 'Synpr', 'Syt2', 'Syt4',\n",
    "       'Sytl4', 'Th', 'Tiparp', 'Tmem108', 'Traf4', 'Ttn', 'Ttyh2'])\n",
    "\n",
    "with open(genetypes_location,'wb') as f:\n",
    "    pickle.dump(dict(ligands=ligands,receptors=receptors,response_genes=response_genes),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a simple experiment: use ligands and receptors to predict response genes in excitatory cells, with a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "nneigh=3\n",
    "ad=anndata.read_h5ad(h5ad_location)\n",
    "connectivity_matrix=anndata.read_h5ad(connectivity_matrix_template%nneigh).X\n",
    "gene_lookup={x:i for (i,x) in enumerate(ad.var.index)}\n",
    "\n",
    "with open(genetypes_location,'rb') as f:\n",
    "    genetypes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot encode cell classes\n",
    "def oh_encode(lst):\n",
    "    lst=np.array(lst)\n",
    "    group_names=np.unique(lst)\n",
    "    group_indexes=np.zeros((len(lst),len(group_names)),dtype=bool)\n",
    "    for i,nm in enumerate(group_names):\n",
    "        group_indexes[lst==nm,i]=True\n",
    "    return group_names,group_indexes\n",
    "cell_classes,cell_class_onehots=oh_encode(ad.obs['Cell_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to construct a prediction problem for a subset of cells\n",
    "\n",
    "def construct_problem(mask,target_gene,neighbor_genes,self_genes):\n",
    "    '''\n",
    "    mask -- set of cells\n",
    "    target_gene -- gene to predict\n",
    "    neighbor_genes -- names of genes which will be read from neighbors\n",
    "    self_genes -- names of genes which will be read from target cell\n",
    "    '''\n",
    "    \n",
    "    # load subset of data relevant to mask\n",
    "    local_processed_expression=np.log1p(ad.X[mask].astype(float)) # get expression on subset of cells\n",
    "    local_edges=connectivity_matrix[mask][:,mask]   # get edges for subset\n",
    "    \n",
    "    selfset_idxs=[gene_lookup[x] for x in self_genes] # collect the column indexes associated with them\n",
    "    selfset_exprs = local_processed_expression[:,selfset_idxs] # collect ligand and receptor expressions\n",
    "       \n",
    "    neighborset_idxs=[gene_lookup[x] for x in neighbor_genes] # collect the column indexes associated with them\n",
    "    neighset_exprs = local_processed_expression[:,neighborset_idxs] # collect ligand and receptor expressions\n",
    "        \n",
    "    n_neighs=(local_edges@np.ones(local_edges.shape[0]))\n",
    "    neigh_avgs = (local_edges@neighset_exprs) / n_neighs[:,None] # average ligand/receptor for neighbors\n",
    "    \n",
    "    neigh_cellclass_avgs = (local_edges@cell_class_onehots[mask]) / n_neighs[:,None] # celltype simplex\n",
    "    \n",
    "    positions=np.array(ad.obs[['Centroid_X','Centroid_Y','Bregma']])[mask] # get positions\n",
    "    \n",
    "    covariates=np.c_[selfset_exprs,neigh_avgs,neigh_cellclass_avgs,positions] # collect all covariates\n",
    "    predict = local_processed_expression[:,gene_lookup[target_gene]] # collect what we're supposed to predict\n",
    "    \n",
    "    excites=(ad.obs['Cell_class']=='Excitatory')[mask] # get the subset of these cells which are excitatory\n",
    "    covariates=covariates[excites] # subset to excites\n",
    "    predict=predict[excites]       # subset to excites\n",
    "    \n",
    "    \n",
    "    return covariates,predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19855, 121) (19855,)\n",
      "(11757, 121) (11757,)\n"
     ]
    }
   ],
   "source": [
    "neighset=genetypes['ligands']\n",
    "oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "# oset=neighset\n",
    "\n",
    "# oset=[]\n",
    "# neighset=[]\n",
    "\n",
    "trainX,trainY=construct_problem((ad.obs['Animal_ID']>=2)&(ad.obs['Animal_ID']<=4),'Ace2',neighset,oset)\n",
    "testX,testY=construct_problem((ad.obs['Animal_ID']==1),'Ace2',neighset,oset)\n",
    "\n",
    "print(trainX.shape,trainY.shape)\n",
    "print(testX.shape,testY.shape)\n",
    "\n",
    "# whiten covariates\n",
    "# mu=np.mean(trainX,axis=0)\n",
    "# sig=np.std(trainX,axis=0)\n",
    "# trainX=(trainX-mu)/sig\n",
    "# testX=(testX-mu)/sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Cbln1', 'Cxcl14', 'Cbln2', 'Vgf', 'Scg2', 'Cartpt', 'Tac2',\n",
       "       'Bdnf', 'Bmp7', 'Cyr61', 'Fn1', 'Fst', 'Gad1', 'Ntng1', 'Pnoc',\n",
       "       'Selplg', 'Sema3c', 'Sema4d', 'Serpine1', 'Adcyap1', 'Cck', 'Crh',\n",
       "       'Gal', 'Gnrh1', 'Nts', 'Oxt', 'Penk', 'Sst', 'Tac1', 'Trh', 'Ucn3'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17421921228055648"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=sklearn.linear_model.Ridge(alpha=1.0)\n",
    "model.fit(trainX,trainY)\n",
    "np.mean(np.abs(model.predict(testX)-testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.017 GB of training data: 0.368 s\n",
      "Binning 0.002 GB of validation data: 0.012 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[2/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[3/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[4/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[5/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[6/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[7/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[8/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[9/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[10/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[11/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[12/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[13/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[14/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[15/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[16/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[17/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[18/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[19/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[20/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[21/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[22/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[23/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[24/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[25/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[26/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[27/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[28/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[29/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[30/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[31/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[32/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[33/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[34/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[35/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[36/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[37/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[38/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[39/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[40/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[41/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[42/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[43/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[44/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[45/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[46/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[47/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[48/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[49/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[50/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[51/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[52/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[53/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[54/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[55/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[56/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[57/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[58/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[59/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[60/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[61/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[62/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[63/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[64/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[65/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[66/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[67/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[68/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[69/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[70/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[71/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[72/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[73/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[74/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[75/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[76/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[77/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[78/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[79/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[80/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[81/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[82/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[83/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[84/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[85/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.008s\n",
      "[86/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[88/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[89/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[90/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[91/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[92/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[93/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[94/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[95/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[96/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[97/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[98/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[99/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "[100/10000] 1 tree, 2 leaves, max depth = 1, train loss: 0.10933, val loss: 0.11145, in 0.007s\n",
      "Fit 100 trees in 1.129 s, (200 total leaves)\n",
      "Time spent computing histograms: 0.151s\n",
      "Time spent finding best splits:  0.032s\n",
      "Time spent applying splits:      0.012s\n",
      "Time spent predicting:           0.003s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10130625125773991"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "model=HistGradientBoostingRegressor(loss=\"absolute_error\", min_samples_leaf=2, verbose=1, random_state=129, max_iter=10000, n_iter_no_change=100)\n",
    "model.fit(trainX,trainY)\n",
    "np.mean(np.abs(model.predict(testX)-testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same 3 cells as above but w/ standardizing this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19855, 121) (19855,)\n",
      "(11757, 121) (11757,)\n"
     ]
    }
   ],
   "source": [
    "neighset=genetypes['ligands']\n",
    "oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "# oset=neighset\n",
    "\n",
    "# oset=[]\n",
    "# neighset=[]\n",
    "\n",
    "trainX,trainY=construct_problem((ad.obs['Animal_ID']>=2)&(ad.obs['Animal_ID']<=4),'Ace2',neighset,oset)\n",
    "testX,testY=construct_problem((ad.obs['Animal_ID']==1),'Ace2',neighset,oset)\n",
    "\n",
    "print(trainX.shape,trainY.shape)\n",
    "print(testX.shape,testY.shape)\n",
    "\n",
    "# whiten covariates\n",
    "mu=np.mean(trainX,axis=0)\n",
    "sig=np.std(trainX,axis=0)\n",
    "trainX=(trainX-mu)/sig\n",
    "testX=(testX-mu)/sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17420094926196253"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=sklearn.linear_model.Ridge(alpha=1.0)\n",
    "model.fit(trainX,trainY)\n",
    "np.mean(np.abs(model.predict(testX)-testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10130625125773991"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "model=HistGradientBoostingRegressor(loss=\"absolute_error\")\n",
    "model.fit(trainX,trainY)\n",
    "np.mean(np.abs(model.predict(testX)-testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Standard Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighset=genetypes['ligands']\n",
    "oset=np.r_[genetypes['ligands'],genetypes['receptors']]\n",
    "# oset=neighset\n",
    "\n",
    "# oset=[]\n",
    "# neighset=[]\n",
    "\n",
    "trainX,trainY=construct_problem((ad.obs['Animal_ID']>=2)&(ad.obs['Animal_ID']<=4),'Ace2',neighset,oset)\n",
    "testX,testY=construct_problem((ad.obs['Animal_ID']==1),'Ace2',neighset,oset)\n",
    "\n",
    "mu=np.mean(trainX,axis=0)\n",
    "sig=np.std(trainX,axis=0)\n",
    "trainX_Jackson=(trainX-mu)/sig\n",
    "testX_Jackson=(testX-mu)/sig\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(trainX)\n",
    "trainX_Roman = scaler.transform(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
